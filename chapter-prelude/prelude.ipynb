{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prelude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\"> \n",
    "\n",
    "The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this work, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the importance of uncertainty quantification steadily grows with the spread of machine learning, there are still few resources that make the topic accesible to a wider range of people. Most of the resources that can be found online are scientific publications and unlike for machine learning textbooks for a broad audience without scientific background are not available. The aim of this book is to change that by providing a comprehensive overview of the topic in an interactive format. Inspired by the [dive into deep learning](https://d2l.ai/index.html) book we tried to include code examples in all chapters of the book that make it easier to understand abstracts concepts and encourage the reader to play around with the examples. The underlying content is based on a joint work of Eyke HÃ¼llermeeir and Willem Waegeman {cite:p}`DBLP:journals/ml/HullermeierW21`. The project has been developed by students at the Institute of Informatics at LMU Munich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content and Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"justify\">\n",
    "\n",
    "The book consists out of three main parts, that focus on discussing the different sources of uncertainty, statistical approaches for modelling approximation uncertainty and machine learning specific methods for representing uncertainty.\n",
    "\n",
    "- **Part 1: Sources of uncertainty in supervised learning.** The distinction of different sources of uncertainty is discussed in {ref}`Chapter 2<know>`.\n",
    "\n",
    "- **Part 2: Modelling approximation uncertainty: Set-based versus distributional representations.** Different statistical approaches for modelling uncertainty are discussed in {ref}`Chapter 4 <sbvd>`.\n",
    "\n",
    "- **Part 3: Machine learning methods for representing uncertainty.** This is the main part of the book, which presents several different machine learning methods that allow for representing a learners uncertainty in a prediction. First, approaches that use classical frequentist statistics for quantifying uncertainty are discussed: Chapters {ref}`5<pe-scoring>`, {ref}`6<pe-calibration>` and {ref}`7<pe-ensemble>` discuss how to estimate probabilities via scoring, calibration and ensembles. {ref}`Chapter 8<fisher>` treat maximum likelihood estimation and the fisher information matrix. {ref}`Chapter 9<genmodels>` discusses generative models. Next, Bayesian approaches for uncertainty quantification are discussed: {ref}`Chapter 10<gaussian-processes>` presents gaussian processes. Chapter {ref}`11<deep-ensembles>` and {ref}`12<bayesian-nn>` describe ensembles of deep neural networks and bayesian neural networks. {ref}`Chapter 13<credal>` addresses the concept of credal sets and {ref}`chapter 14<uqnl>` reliable classification. Lastly, the concept of set valued prediction is introduced. Chapter {ref}`15<conformal-class>` and `16<conformal-reg>` discuss conformal prediction for classification and regression respectively. {ref}`Chapter 17<set-util>` explains set-valued prediction based on utility maximization.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the chapters include code examples implemented in Python. The readers are encouraged to change and play around with them. To do this a chapter can be either opened in google colab (rocket in top right corner) or found as a notebook in the [Github repository](https://github.com/Advueu963/PageTest). The needed inputs are listed in the beginning cell of each chapter, all required libraries can be found in the [requirements.txt](https://github.com/Advueu963/PageTest/blob/main/requirements.txt) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the code examples have been developed for illustration purposes only. While some of the examples use well established libraries like scikit learn for machine learning, we often found ourselves implementing concepts from scratch because there were no existing libraries to the best of our knowledge. While the authors put a lot of effort into developing these code examples a lot of the code is not thoroughly tested for reliability and efficiency in every scenario and should never be used in production environments without performing additional quality controls first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project was build using [Jupyter Book](https://jupyterbook.org/en/stable/intro.html#) and can be downloaded as PDF or [Jupyter NoteBook](https://jupyter.org). \n",
    "Additionally the whole project including all individual notebooks can be found on the projects [Github Page](https://github.com/Advueu963/PageTest). Readers are encouraged to leave feedback and comments in the discussion forum that can be found at the bottom of each page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Audience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book is for students and researchers who want to get started with uncertainty quantification in machine learning or seek a accesible illustration of already known concepts. It is also intended for engineers, who want to learn about ways to build uncertainty aware machine learning models or quantify the uncertainty of existing ones. In general we explain all uncertainty quantification topics from the ground up, so no background knowledge in that field is required. However, since the aim of this book is to merely discuss topics of uncertainty quantification not to introduce basic machine learning concepts. \n",
    "We therefore assume the reader enters with some prior knowledge about machine learning as well as basic knowledge of probability theory and Python programming."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
