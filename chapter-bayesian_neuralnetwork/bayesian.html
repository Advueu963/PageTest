
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>12. Bayesian Neural Networks &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/myStyle.css?v=e90502a2" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": ["\\mathbb{N}"], "vec": ["\\boldsymbol{#1}", 1], "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"], "defeq": ["\\mathrel{\\vcenter{\\baselineskip0.5ex \\lineskiplimit0pt \\hbox{\\footnotesize.}\\hbox{\\footnotesize.}}}% ="], "given": ["\\, | \\,"], "cX": ["\\mathcal{X}"], "cY": ["\\mathcal{Y}"], "cH": ["\\mathcal{H}"], "cD": ["\\mathcal{D}"], "hath": ["\\hat{h}"], "haty": ["\\hat{y}"], "hatp": ["\\hat{p}"], "sety": ["\\widehat{Y}"], "argmin": ["\\operatorname*{argmin}"], "argmax": ["\\operatorname*{argmax}"], "db": ["\\set{M}"], "fkt": ["#1(\\cdot)", 1], "chrfkt": ["\\mathbb{I}_{#1}", 1], "kref": ["(\\ref{#1})", 1], "convto": ["(\\rightarrow"], "fft": ["(#1 :  #2 \\rightarrow #3", 3], "with": ["\\,  | \\,"], "sothat": ["\\,  : \\,"], "defi": ["\\stackrel{\\on{df}}{=}"], "set": ["\\mathcal{#1}", 1], "Prob": ["P"], "prob": ["p"], "impl": ["\\Rightarrow"], "on": ["\\operatorname"], "groesser": ["\\raisebox{#1mm}{} \\raisebox{-#1mm}{}", 1], "sgroesser": ["\\groesser{1.20}"], "xleftr": ["\\left( \\groesser{1.35} "], "xleftg": ["\\left\\{ \\groesser{1.35} "], "fftm": ["\\fft{#1}{#2}{#3} \\, ,\\, #4 \\mapsto #5", 5], "gdw": ["\\Leftrightarrow"], "gdwbd": ["\\stackrel{\\on{df}}{\\Leftrightarrow}"], "est": ["{est}"], "epd": ["\\Leftrightarrow_{\\on{def}}"], "fromto": ["\\longrightarrow"], "pref": ["\\succ"], "evalue": ["\\mathbf{E}"], "variance": ["\\mathbf{V}"], "mmp": ["", 1], "llbracket": ["\\lbrack\\lbrack"], "rrbracket": ["\\rbrack\\rbrack"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter-bayesian_neuralnetwork/bayesian';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="13. Credal Sets and Classifiers" href="../chapter-credal_sets/credal_sets.html" />
    <link rel="prev" title="11. Deep Neural Network Ensembles" href="../chapter-deep_neuralnetwork/dnn.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../startPage.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../startPage.html">
                    Toolbox for Uncertainty Quantification in Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter-prelude/prelude.html">1. Prelude</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-intro/intro.html">2. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-srcUncertainty/src.html">3. Sources of uncertainty in supervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-modelingAproxUncertainty/modelingAproxUncertainty.html">4. Modelling Approximation Uncertainty</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-pe-scoring/scoring.html">5. Probability Estimation via Scoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-pe-calibration/calibration.html">6. Probability Estimation and Calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-pe-ensemble/ensemble.html">7. Probability Estimation and Ensembles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-mle_and_fisher/mle.html">8. Maximum Likelihood and Fisher Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-generative_models/gm.html">9. Generative Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-gaussianprocess/gaussianprocess.html">10. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-deep_neuralnetwork/dnn.html">11. Deep Neural Network Ensembles</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">12. Bayesian Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-credal_sets/credal_sets.html">13. Credal Sets and Classifiers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-reliable_classification/reliable_classification.html">14. Reliable Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-conformel_classification/conformel_classification.html">15. Conformal Prediction for Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-conformel_regression/conformel_regression.html">16. Conformal Prediction for Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-setValued_utilityMaximization/set.html">17. Set-valued Prediction Based on Utility Maximization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-appendix/appendix.html">18. Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">19. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/Advueu963/PageTest/blob/main/chapter-bayesian_neuralnetwork/bayesian.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Advueu963/PageTest" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Advueu963/PageTest/edit/main/chapter-bayesian_neuralnetwork/bayesian.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Advueu963/PageTest/issues/new?title=Issue%20on%20page%20%2Fchapter-bayesian_neuralnetwork/bayesian.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter-bayesian_neuralnetwork/bayesian.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bayesian Neural Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">12.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-monte-carlo-methods">12.2. Markov Chain Monte Carlo Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-methods">12.3. Variational Methods</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bayesian-neural-networks">
<h1><span class="section-number">12. </span>Bayesian Neural Networks<a class="headerlink" href="#bayesian-neural-networks" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2><span class="section-number">12.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>For this chapter we need some background knowledge of basic bayesian statistics as well as deep learning. For Deep Learning you might want to check out the introductory book by <span id="id1">[<a class="reference internal" href="../references.html#id2293" title="Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, Cambridge, MA, USA, 2016. http://www.deeplearningbook.org.">GBC16b</a>]</span> which is <a class="reference external" href="https://www.deeplearningbook.org/">available online</a> or the book by  <span id="id2">[<a class="reference internal" href="../references.html#id2294" title="Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola. Dive into deep learning. CoRR, 2021. URL: http://dblp.uni-trier.de/db/journals/corr/corr2106.html#abs-2106-11342.">ZLLS21</a>]</span>, which is <a class="reference external" href="https://d2l.ai/">also available online</a> and provides additional code examples.
The ideas of Monte Carlo methods in this chapter are mainly taken from the book by <span id="id3">[<a class="reference internal" href="../references.html#id2296" title="C.P. Robert and G. Casella. Monte Carlo statistical methods. Springer Verlag, 2004.">RC04</a>]</span>. You might also want to check out the book by <span id="id4">[<a class="reference internal" href="../references.html#id2295" title="A.A. Johnson, M.Q. Ott, and M. Dogucu. Bayes Rules!: An Introduction to Applied Bayesian Modeling. Chapman &amp; Hall/CRC texts in statistical science. CRC Press, 2022. ISBN 9781032191591. URL: https://books.google.de/books?id=pISQzgEACAAJ.">JOD22</a>]</span> which has an <a class="reference external" href="https://www.bayesrulesbook.com/">online version</a> as well. The paper by <span id="id5">[<a class="reference internal" href="../references.html#id2297" title="Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed Bennamoun. Hands-on bayesian neural networks—a tutorial for deep learning users. IEEE Computational Intelligence Magazine, 17(2):29-48, 2022. doi:10.1109/MCI.2022.3155327.">JLB+22</a>]</span> provides an introduction to the specifics of Bayesian Neural Nets (BNN), assuming some knowledge in Neural Networks. To start with recall bayes formula:</p>
<div class="math notranslate nohighlight">
\[
\underbrace{p(h|\mathcal{D})}_{Posterior} = \frac{\overbrace{p(h)}^{Prior} \cdot \overbrace{p(\mathcal{D}|h)}^{Likelihood}}{\underbrace{p(\mathcal{D})}_{Evidence/Marginal}}
\]</div>
<p>Also recall that in a bayesian setting we do not assume an unknown but fixed parameter but instead a distribution over parmeter(s) of interest. For Neural networks our hypothesis consists of a network architecture including weights, activation functions,  bias etc. but for now let us fix the overall architecture and let us consider the case of a simple Feed Forward Neural Networks (FNN) which can be considered a compostion of several functions:</p>
<p><span class="math notranslate nohighlight">\(\hat{y} = f_{k} \circ f_{k-1} \circ f_{1} \circ ( {\bf{x}} )= f_{k}(f_{k-1}( \dotsm (f_{1}( {\bf{x}} ))) := \psi( {\bf{x}})\)</span> for <span class="math notranslate nohighlight">\({\bf{x}} \in \mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(\hat{y} \in \mathcal{Y}\)</span>.</p>
<p>Typically this function is parametrized by some parameters <span class="math notranslate nohighlight">\({\bf{\theta}}\)</span>. To indicate this one can write more precisely <span class="math notranslate nohighlight">\(\phi_{ \bf{\theta} }( {\bf{x}} )\)</span> (subscript notation is used in order avoid ‘|’ notation for bayesian conditioning). For Neural Networks these parameters are the weights and biases.Instead of assuming a fixed scalar quantity for each bias and weight term we assume a realization from some distribution to be specified upfront, e.g. if we denote the weights between unit i and j with <span class="math notranslate nohighlight">\(w_{i,j}\)</span> we assume <span class="math notranslate nohighlight">\(w_{i,j} \sim G_{i,j}\)</span> for some distribution <span class="math notranslate nohighlight">\(G_{i,j}\)</span> (the analog holds for the bias term as well). One could also impose a probability distribution on the activations instead of the weights but both are valid choices of it own and one can be written in terms of the other (see <span id="id6">[<a class="reference internal" href="../references.html#id2297" title="Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed Bennamoun. Hands-on bayesian neural networks—a tutorial for deep learning users. IEEE Computational Intelligence Magazine, 17(2):29-48, 2022. doi:10.1109/MCI.2022.3155327.">JLB+22</a>]</span> p. 6).
Reformulating Bayes rue from above with weight notation we have:</p>
<div class="math notranslate nohighlight">
\[
\underbrace{p({\bf{w}} |\mathcal{D})}_{\sim epistemic} = \frac{p( {\bf{w}} ) \cdot \overbrace{p(\mathcal{D}| {\bf{w}} )}^{\sim aleatoric}}{p(\mathcal{D})}
\]</div>
<p>Choosing an appropriate posterior distribution one weights and biases can be difficult and there is still debate on how to choose an appropriate one. Typically a Gaussian is a common choice for BNNs due to desirable asymptotic properties (see  the introcutiory chapter of <span id="id7">[<a class="reference internal" href="../references.html#id2299" title="Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, Berlin, Heidelberg, 1996. ISBN 0387947248.">Nea96</a>]</span>).</p>
<p>Our aim is to quantify the uncertainty associated with neural network predictions. The bayesian reformulation with a weight distribution together with the observed data distribution in form of the Likelihood allows us to decompose uncertainty:</p>
<p>The <em>aleatoric</em> uncertainity is associated with the likelihood since it captures the inherent noise associated with (incomplete) data we observe. The <em>epistemic</em> uncertainity is represented by the posterior distribution.
To calculate the posterior we need to compute the evidence (the denominator of Bayes Rule), which involves integration over a (typically) high dimensional distribution:</p>
<div class="math notranslate nohighlight">
\[
p(\mathcal{D}) = \int_{W}p(\mathcal{D} \lvert {\bf{w}} )p({\bf{w}})d\bf{w}
\]</div>
<p>We integrate over the whole multidimensional space of all weights <span class="math notranslate nohighlight">\({\mathcal{W}}\)</span>, typically this is only tractable for small dimensions. This is a common issue in bayesian statistics for which two appoaches are often used for solving: the first one is based on Markov Chain Monte Carlo (MCMC) methods which provides asymptotically exact results (for proper specified transition kernels), the other one is based on Variational Methods which restates the integration as an optimization problem for some variational distribution which depends itsef on certain parameters and is only an approximation without guarantee of caputuring the exact posterior.</p>
<p>Note that the posterior refers to the underlying parameters, here the weights and biases from some parameter space <span class="math notranslate nohighlight">\(\mathcal{W}\)</span>, and not to the actual variable of interest <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span> . Rather the posterior is used in a next step to compute the <strong>posterior predictive distribution</strong>:</p>
<div class="math notranslate nohighlight">
\[\mathbb{P}(Y^{\prime}|\mathcal{D}) = \int_{\mathcal{W}}\mathbb{P}(Y^{\prime}|{\bf w}) \underbrace{\mathbb{P}({\bf w}|\mathcal{D})}_{\text{posterior}}d {\bf w}\]</div>
<p>One can see that the posterior predictive distribution weights the posterior by the probability of observing <span class="math notranslate nohighlight">\(Y^{\prime}\)</span> over all weights in our weight space <span class="math notranslate nohighlight">\(\mathcal{W}\)</span></p>
<p>Having calulated the posterior distribution, assessing <em>epistemic</em> uncertainty can then be performed by drawing samples from the posterior (predictive) distribution.</p>
<p>Since weights and biases are random variables, training a neural network is now somewhat different. Following the notation of <span id="id8">[<a class="reference internal" href="../references.html#id2297" title="Laurent Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed Bennamoun. Hands-on bayesian neural networks—a tutorial for deep learning users. IEEE Computational Intelligence Magazine, 17(2):29-48, 2022. doi:10.1109/MCI.2022.3155327.">JLB+22</a>]</span>(p.3) with minor adjustments the training can be stated in the following pseudocode of <a class="reference internal" href="#bnn-algorithm">Algorithm 12.1</a>.</p>
<div class="proof algorithm admonition" id="bnn-algorithm">
<p class="admonition-title"><span class="caption-number">Algorithm 12.1 </span> (BNN Inference with stochastic weights and biases)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Require:</strong> prior distribution <span class="math notranslate nohighlight">\(p( {\bf{w}} )\)</span>, Likelihood <span class="math notranslate nohighlight">\(p(\mathcal{D} \lvert {\bf w}), Data \{{\bf{x}}_{i}\}_{i=1}^{N}\)</span></p>
<ol class="arabic">
<li><p>Compute <span class="math notranslate nohighlight">\(p(\bf{w}|\mathcal{D}) = \frac{p( {\bf{w}}) \cdot p(\mathcal{D}|{\bf{w}})}{p(\mathcal{D})}\)</span></p></li>
<li><p>For i=0 to N do:</p>
<p>i.  Draw <span class="math notranslate nohighlight">\( {\bf{w}}_{i} \sim p( {\bf{w}} \vert \mathcal{D}) \)</span></p>
<p>ii. <span class="math notranslate nohighlight">\(\hat{y}_{i} = \phi_{{\bf w}_{i}}({\bf x})\)</span></p>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(\{\hat{y}_{i}\}_{i=1}^{N}, \{{\bf{w}}_{i=1}\}^{N}\)</span></p></li>
</ol>
</section>
</div><p>Since <span class="math notranslate nohighlight">\(\{{\bf{y}}_{i} \}_{i=1}^{N}\)</span> are draws from some distribution the end resut is not a single (best) prediction but rather a distribution over possible predictions we would get for different realizations of <span class="math notranslate nohighlight">\(\bf w\)</span>. To get a single prediction one can take the value with highest probability, e.g. the Maximum aposteriori prediction (MAP).</p>
</section>
<section id="markov-chain-monte-carlo-methods">
<span id="mcmc"></span><h2><span class="section-number">12.2. </span>Markov Chain Monte Carlo Methods<a class="headerlink" href="#markov-chain-monte-carlo-methods" title="Link to this heading">#</a></h2>
<p>Our aim is to take sample from the posterior distribution but as was pointed out above computation can be hard, especially if we are in the unlucky and more realistic case of no conjugancy. The idea of MCMC is to draw samples from a sequence of random variables that will under certain conditions asymptotically converege to the desired distribution of interest. This sequence is called a Markov Chain:</p>
<div class="proof definition admonition" id="mc-def">
<p class="admonition-title"><span class="caption-number">Definition 12.1 </span> (Markov Chain)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(S:=X_{1},X_{2},...\)</span> be a sequence of random variables.
<span class="math notranslate nohighlight">\(S\)</span> is called a Markov Chain if <span class="math notranslate nohighlight">\(\mathbb{P}(X_{i}|X_{1},X_{2},..X_{i-1}) = \mathbb{P}(X_{i}|X_{i-1})\)</span> <span class="math notranslate nohighlight">\(\forall\)</span> <span class="math notranslate nohighlight">\(i\)</span> <span class="math notranslate nohighlight">\(\in\)</span> <span class="math notranslate nohighlight">\(\{2,3,...\}\)</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(X_{1})\)</span> is called the <strong>initial distribution</strong>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{P}(X_{i}|X_{i-1})\)</span> is called the <strong>transition probability distribution</strong>.</p></li>
<li><p>The support of <span class="math notranslate nohighlight">\(X_{i}\)</span>’s is called the <strong>state space</strong> of the Markov Chain</p></li>
</ul>
</section>
</div><p>A Markov Chain can have certain important properties which are stated below. A tourough treatment can be found in <span id="id9">[<a class="reference internal" href="../references.html#id2296" title="C.P. Robert and G. Casella. Monte Carlo statistical methods. Springer Verlag, 2004.">RC04</a>]</span> on page 207 onwards.</p>
<ul class="simple">
<li><p><strong>Stationarity</strong>: <span class="math notranslate nohighlight">\(\forall k,i\)</span> <span class="math notranslate nohighlight">\(\in\)</span> <span class="math notranslate nohighlight">\(\{1,2,..\}\)</span>, the sequence <span class="math notranslate nohighlight">\((X_{i+1},...,X_{i+k})\)</span> does not depend on the choice of <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p><strong>Reversibility</strong>: The distribution of pairs <span class="math notranslate nohighlight">\((X_{i}, X_{i+1})\)</span> from <span class="math notranslate nohighlight">\(S\)</span> is exchangeable.</p></li>
<li><p><strong>Recurrence</strong>: Each state <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> will be reached an infinite amount of times in expectation.</p></li>
</ul>
<p>Reversibility implies Stationarity which ensures that for some <span class="math notranslate nohighlight">\(i\)</span> we can make the claim that if <span class="math notranslate nohighlight">\(X_{i}\)</span> has a certain distribution, say <span class="math notranslate nohighlight">\(\pi\)</span> the so does <span class="math notranslate nohighlight">\(X_{i+1}\)</span>, e.g. <span class="math notranslate nohighlight">\(X_{i+1} \sim \pi\)</span> (the stationary distribution is therefore also the limiting distribution). An import choice for achieving this is to select a suitable transition probability (also called Kernel). Especially if the Kernel is <em>irreducible</em> (roughly speaking all possible states can be reached in finite steps) stationarity can be concluded. One of the most prominent versions of MCMC sampling is the Metropolis Hastings algorithms:</p>
<div class="proof algorithm admonition" id="MH-algo">
<p class="admonition-title"><span class="caption-number">Algorithm 12.2 </span> (Metropolis Hastings)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong> Given initial value <span class="math notranslate nohighlight">\(X^{0}\)</span>, target distribution <span class="math notranslate nohighlight">\(f\)</span>, Kernel <span class="math notranslate nohighlight">\(q\)</span></p>
<p><strong>Output</strong> Sequence of random variables <span class="math notranslate nohighlight">\((X^{n}, X^{n+1},...)\)</span></p>
<ol class="arabic">
<li><p>Until convergence criterion met:</p></li>
<li><p>Draw <span class="math notranslate nohighlight">\(Y_{t}\)</span> from <span class="math notranslate nohighlight">\(q(y|x^{(t)})\)</span></p>
<p>2.1 Calculate acceptance probability  <span class="math notranslate nohighlight">\(\alpha(x,y) := \min{\left(1, \frac{f(y) \times q(x|y)}{f(x) \times q(y|x)} \right)}\)</span></p>
<p>2.2 Drawn random uniform number <span class="math notranslate nohighlight">\(u \sim U(0,1)\)</span></p>
<p>2.3 If <span class="math notranslate nohighlight">\(\alpha \geq u\)</span> :Set <span class="math notranslate nohighlight">\(X^{(t+1)} = Y_{t}\)</span></p>
<p>2.5 Else: <span class="math notranslate nohighlight">\(X^{(t+1)} = x^{(t)}\)</span></p>
</li>
</ol>
</section>
</div><p>Altough for the Metropolis Hasting algorithms convergence <em>guarantees</em> can be shown, a typical issue is convergence <em>rate</em> since it might just take too long until we get draws that can be considered to  be from the desired target distribution. Hamiltonian MCMC methods and adaptions thereof like the <em>NUTS-Sampler</em> try to solve this issue and are presented briefly. For a more tourough treatment and reference to related literature one can read into the STAN user manual which is <a class="reference external" href="https://mc-stan.org/docs/reference-manual/mcmc.html">available online</a>.
The idea of Hamiltonian MCMC is to introduce a momentum variable <span class="math notranslate nohighlight">\(\rho\)</span> to speed up the process. If we denote the posterior distribution of parameters <span class="math notranslate nohighlight">\(\theta\)</span> and momentum by <span class="math notranslate nohighlight">\(p(\theta)\)</span> we can write the negative logarithm as <span class="math notranslate nohighlight">\(p(\rho, \theta) = p(\rho|\theta) p(\theta) = T(\rho|\theta) + V(\theta)\)</span> by using logarithm laws and proper definition of <span class="math notranslate nohighlight">\(T\)</span> and <span class="math notranslate nohighlight">\(V\)</span> (to get the gist of the method we stick to the univariate case). For some given initial value of <span class="math notranslate nohighlight">\(\theta\)</span> a simultaneous update for <span class="math notranslate nohighlight">\(\theta\)</span> and <span class="math notranslate nohighlight">\(\rho\)</span> can be phrased as a system of two differential equations which can be solved using the leapfrog algorithm, which requires a discretization step <span class="math notranslate nohighlight">\(\epsilon\)</span> and a stepsize <span class="math notranslate nohighlight">\(L\)</span> as input. Due to possible numerical errors in this step the newly calculated values of <span class="math notranslate nohighlight">\(\rho\)</span> and <span class="math notranslate nohighlight">\(\theta\)</span> are accepted only with a certain probability, similar to the MH algorithm above. The idea of the NUTS-sampler now is to adaptively set <span class="math notranslate nohighlight">\(\epsilon\)</span> and <span class="math notranslate nohighlight">\(L\)</span> in each iteration, typically handled differently in an early warm-up phase and thereafter leading to faster convergence results.</p>
<p>In the following we will consider the wine data; we will implement our bayesian neural networks using <code class="docutils literal notranslate"><span class="pre">PyMc</span></code> and sample from the predictive distribution to asses epistemic uncertainty:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">pytensor</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_wine</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Running on PyMC v</span><span class="si">{</span><span class="n">pm</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running on PyMC v5.15.1
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># set general parameters</span>
<span class="n">SEED</span> <span class="o">=</span> <span class="mi">123</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_default_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="n">mpl</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">floatX</span> <span class="o">=</span> <span class="n">pytensor</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_wine</span><span class="p">,</span> <span class="n">y_wine</span> <span class="o">=</span> <span class="n">load_wine</span><span class="p">(</span><span class="n">return_X_y</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">X_train_wine</span><span class="p">,</span> <span class="n">X_test_wine</span><span class="p">,</span> <span class="n">y_train_wine</span><span class="p">,</span> <span class="n">y_test_wine</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_wine</span><span class="p">,</span> <span class="n">y_wine</span><span class="p">,</span>
                                                                        <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                                        <span class="n">random_state</span> <span class="o">=</span> <span class="n">SEED</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here again we define the Wine model we encountered in the Deep Ensemble chapter. We will need the learned weights to specifiy the expected values for our distributions. For convenience we retrain and state the model again:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">WineModel</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WineModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">X_train_wine</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">30</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">nn</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">classification</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span> <span class="n">bootstrap</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">boot_size</span> <span class="o">=</span> <span class="mf">0.75</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Train a singe DNN.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># create boostrap sample</span>
    <span class="k">if</span> <span class="n">bootstrap</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">boot_size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">boot_size</span> <span class="o">&lt;</span> <span class="mi">1</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">n_boot</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">boot_size</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="p">))))</span>
        <span class="n">idx_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">idpredict_modelx</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="n">n_boot</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">idx_boot</span><span class="p">]</span>
        <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">idx_boot</span><span class="p">]</span>

    <span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
    
    <span class="k">if</span> <span class="n">classification</span><span class="p">:</span>
        <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
        <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="c1"># train for n_epochs</span>
    <span class="n">acc_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    
        <span class="c1"># Forward pass</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">acc_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    
        <span class="c1"># Backward pass and optimization</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">acc_loss</span>
    
<span class="k">def</span> <span class="nf">predict_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">classification</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">classification</span><span class="p">:</span>
           <span class="n">pred</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pred</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nn_wine</span> <span class="o">=</span> <span class="n">WineModel</span><span class="p">()</span>
<span class="n">loss_wine</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span><span class="n">nn_wine</span><span class="p">,</span> <span class="n">X_train_wine</span><span class="p">,</span> <span class="n">y_train_wine</span><span class="p">,</span> <span class="n">bootstrap</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">wine_train_preds</span> <span class="o">=</span> <span class="n">predict_model</span><span class="p">(</span><span class="n">nn_wine</span><span class="p">,</span> <span class="n">X_train_wine</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Just for a sanity check, let us see whether we did actually sucessfully learn on the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_wine</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loss for Wine data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/73ca252640eda90ac39ecb1e142db8f318aae863110c7ae8f35b788699679cf0.png" src="../_images/73ca252640eda90ac39ecb1e142db8f318aae863110c7ae8f35b788699679cf0.png" />
</div>
</div>
<p>Okay, the loss seems to converge to some minimum. Now let us see how our predicted values of wine classes are distributed compared to the original values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fix</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="n">_</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train_wine</span><span class="p">,</span> <span class="n">return_counts</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Frequency&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Observed&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">wine_train_preds</span><span class="p">,</span> <span class="n">return_counts</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Predicted&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d8ade2baae2dd0e1ad0efd4d2e355a831b3c1403178a54be25a02ebf040adc1f.png" src="../_images/d8ade2baae2dd0e1ad0efd4d2e355a831b3c1403178a54be25a02ebf040adc1f.png" />
</div>
</div>
<p>Our model seems to have learned sucessfully since training loss went down and the predicted class distribution of our training data resembles the original. Let us now store the weight parameters in the variable <code class="docutils literal notranslate"><span class="pre">wine_state</span></code> and go on with the bayesian version of our trained network:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wine_state</span> <span class="o">=</span> <span class="n">nn_wine</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>In PyMc we have the option to label dimensions which will make auto generated plots more readable. Therefore we set coordinates for the size of each hidden layer ‘h_&lt;layer position’ as well as for our observations and the training colums.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coords_wine</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;h_1&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">wine_state</span><span class="p">[</span><span class="s2">&quot;fc1.weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
    <span class="s2">&quot;h_2&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">wine_state</span><span class="p">[</span><span class="s2">&quot;fc2.weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
    <span class="s2">&quot;h_3&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">wine_state</span><span class="p">[</span><span class="s2">&quot;fc3.weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
    <span class="s2">&quot;train_cols&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X_train_wine</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
    <span class="s2">&quot;obs_id&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X_train_wine</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<p>A critical choice is the variance of the weights: we assumed each weight is normally distributed where the expected value equals the weight value we extracted from the optimized model. Since the weights are typically in ranges around zero a too high variance would too much. Therefore we choose a variance of 0.05 for each normal. The model needs to be defined in a certain contex which specifies the relation between the random variables. In our case the layer architecture:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scale</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">coords</span> <span class="o">=</span> <span class="n">coords_wine</span><span class="p">)</span> <span class="k">as</span> <span class="n">bnn_wine</span><span class="p">:</span>

    <span class="n">ann_input</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Data</span><span class="p">(</span><span class="s2">&quot;ann_input&quot;</span><span class="p">,</span> <span class="n">X_train_wine</span><span class="p">,</span> <span class="n">dims</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;obs_id&quot;</span><span class="p">,</span> <span class="s2">&quot;train_cols&quot;</span><span class="p">))</span>
    <span class="n">ann_output</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Data</span><span class="p">(</span><span class="s2">&quot;ann_output&quot;</span><span class="p">,</span> <span class="n">y_train_wine</span><span class="p">,</span> <span class="n">dims</span> <span class="o">=</span> <span class="s2">&quot;obs_id&quot;</span><span class="p">)</span>

    <span class="c1"># set expected value and variance for normals</span>
    <span class="c1"># each entry of weight matrix is a normal, no correlation</span>
    <span class="n">mu_w_1</span> <span class="o">=</span> <span class="n">wine_state</span><span class="p">[</span><span class="s2">&quot;fc1.weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">mu_b_1</span> <span class="o">=</span> <span class="n">wine_state</span><span class="p">[</span><span class="s2">&quot;fc1.bias&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">sigma_w_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">mu_w_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="n">sigma_b_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">mu_b_1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="n">mu_w_2</span> <span class="o">=</span> <span class="n">wine_state</span><span class="p">[</span><span class="s2">&quot;fc2.weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">mu_b_2</span> <span class="o">=</span> <span class="n">wine_state</span><span class="p">[</span><span class="s2">&quot;fc2.bias&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">sigma_w_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">mu_w_2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>
    <span class="n">sigma_b_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">mu_b_2</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="n">mu_w_3</span> <span class="o">=</span> <span class="n">wine_state</span><span class="p">[</span><span class="s2">&quot;fc3.weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">sigma_w_3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">mu_w_3</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale</span>

    <span class="c1"># create normal distributed weights and biases</span>
    <span class="n">w_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;w_1&quot;</span><span class="p">,</span> <span class="n">mu_w_1</span><span class="p">,</span> <span class="n">sigma_w_1</span><span class="p">,</span> <span class="n">dims</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;train_cols&quot;</span><span class="p">,</span> <span class="s2">&quot;h_1&quot;</span><span class="p">))</span>
    <span class="n">b_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;b_1&quot;</span><span class="p">,</span> <span class="n">mu_b_1</span><span class="p">,</span> <span class="n">sigma_b_1</span><span class="p">,</span> <span class="n">dims</span> <span class="o">=</span> <span class="s2">&quot;h_1&quot;</span><span class="p">)</span>
    <span class="n">w_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;w_2&quot;</span><span class="p">,</span> <span class="n">mu_w_2</span><span class="p">,</span> <span class="n">sigma_w_2</span><span class="p">,</span> <span class="n">dims</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;h_1&quot;</span><span class="p">,</span> <span class="s2">&quot;h_2&quot;</span><span class="p">))</span>
    <span class="n">b_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;b_2&quot;</span><span class="p">,</span> <span class="n">mu_b_2</span><span class="p">,</span> <span class="n">sigma_b_2</span><span class="p">,</span> <span class="n">dims</span> <span class="o">=</span> <span class="s2">&quot;h_2&quot;</span><span class="p">)</span>
    <span class="n">w_3</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;w_3&quot;</span><span class="p">,</span> <span class="n">mu_w_3</span><span class="p">,</span> <span class="n">sigma_w_3</span><span class="p">,</span> <span class="n">dims</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;h_2&quot;</span><span class="p">,</span> <span class="s2">&quot;h_3&quot;</span><span class="p">))</span>


    <span class="n">act_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;act_1&quot;</span><span class="p">,</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ann_input</span><span class="p">,</span> <span class="n">w_1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_1</span><span class="p">))</span>
    <span class="n">act_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;act_2&quot;</span><span class="p">,</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">act_1</span><span class="p">,</span> <span class="n">w_2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b_2</span><span class="p">))</span>
    <span class="n">act_3</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s2">&quot;act_3&quot;</span><span class="p">,</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">act_2</span><span class="p">,</span> <span class="n">w_3</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="s2">&quot;out&quot;</span><span class="p">,</span><span class="n">act_3</span><span class="p">,</span> <span class="n">observed</span> <span class="o">=</span> <span class="n">ann_output</span><span class="p">,</span> <span class="n">dims</span> <span class="o">=</span> <span class="s2">&quot;obs_id&quot;</span><span class="p">)</span>
    
</pre></div>
</div>
</div>
</div>
<p>We can also make use of the <code class="docutils literal notranslate"><span class="pre">arviz</span></code> package and visualize the conceptual setup of our model in so called plate notation (explanation below). We use the letter ‘w’ to denote a weight random variable and ‘b’ to denote a bias random variable.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># pm.model_graph.model_to_graphviz(model = bnn_wine, save = &quot;bnn_wine_graph.jpg&quot;)</span>
</pre></div>
</div>
</div>
</details>
</div>
<p><img alt="image" src="../_images/bnn_wine_graph.jpg" /></p>
<p>Each ‘plate’ indicates a repetition of data. Since we have 177 training observations we have 177 repetitions. A round circle represents a latent random variable which is unobserved (weights and biases), a grey circle an observed random variable (input and output data) and a rectangle some deterministic entity without distributional assumptions.</p>
<p>We can make use of build in random variables generators and sample weights, say 500 from a normal for each component:</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>

<span class="k">with</span> <span class="n">bnn_wine</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>
    <span class="c1"># by convention the created inference data is often called trace or idata (sorthand for &#39;inference data&#39;)</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Auto-assigning NUTS sampler...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Multiprocess sampling (2 chains in 2 jobs)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NUTS: [w_1, b_1, w_2, b_2, w_3]
</pre></div>
</div>
<div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">/opt/hostedtoolcache/Python/3.12.4/x64/lib/python3.12/site-packages/rich/live.py:231: UserWarning: install 
"ipywidgets" for Jupyter support
  warnings.warn('install "ipywidgets" for Jupyter support')
</pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"></pre>
</div><div class="output text_html"><pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
</pre>
</div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 2 chains for 1_000 tune and 500 draw iterations (2_000 + 1_000 draws total) took 68 seconds.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>We recommend running at least 4 chains for robust computation of convergence diagnostics
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 7.8 s, sys: 372 ms, total: 8.17 s
Wall time: 1min 43s
</pre></div>
</div>
</div>
</details>
</div>
<p>There are several things to notice. First, by default the <em>NUTS-sampler</em> is used. Second, notice that the computation did actually take quite some time, considering we have a fairly small model and take only a few samples. Also, we draw 4 times a sample sequence, a common choice in MCMC setting for diagnostic reasons. Now lets take a look at the result. For simplicity only the bias terms are shown since we have too many weights in each layer to get a clear picture that is not too cluttered. On the left the estiated density from the samples is plotted and on the right sampled values from each chain.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span> <span class="p">[</span><span class="s2">&quot;b_1&quot;</span><span class="p">,</span> <span class="s2">&quot;b_2&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/41b2c77233de5844b31f84b443a9b2d8f200f8782667b8ad0532720fda91d7de.png" src="../_images/41b2c77233de5844b31f84b443a9b2d8f200f8782667b8ad0532720fda91d7de.png" />
</div>
</div>
<p>What we see is what we would expect: each bias term is norally distributed with mean given by the previously trained model and some given variance (which is the same for all bias terms).<br>
On the right side we can see the realizations of each draw for each bias term. Since we do not see any autocorrelation and for each bias term it appears to be a random draw around its mean so we do not go any deeper in diagnositic checks that would be typically done in MCMC methods.
Next we can calculate <em>posterior predictive</em> quantitites based on our posterior distribution over the weights and biases:</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">bnn_wine</span><span class="p">:</span>
    <span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
    <span class="n">trace</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">ppc</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If we did set up the mean values for our weights corecctly the posterior predictive distribution should resemble the actual distribution in our training data since (since our model predicted fairly well).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_dist</span><span class="p">(</span><span class="n">trace</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s2">&quot;out&quot;</span><span class="p">],</span> <span class="n">kind</span> <span class="o">=</span> <span class="s2">&quot;hist&quot;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Posterior Predictive distribution&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;relative Frequency&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train_wine</span><span class="p">,</span> <span class="n">return_counts</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Actual data distribution&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/36e385c02c3758d9d15bc95ce30e2737574cecb2fa0a065cfc1b209678ef1531.png" src="../_images/36e385c02c3758d9d15bc95ce30e2737574cecb2fa0a065cfc1b209678ef1531.png" />
</div>
</div>
<p>To further stress the point that we are actually facing a distribution over predicted values and not a single prediction let us consider  two datapoints: An hitherto unobserved test point and the first encountered training point.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Actual class test data point:      </span><span class="si">{</span><span class="n">y_test_wine</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predicted class: test data point:  </span><span class="si">{</span><span class="n">predict_model</span><span class="p">(</span><span class="n">nn_wine</span><span class="p">,</span><span class="w"> </span><span class="n">X_test_wine</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Actual class first training point: </span><span class="si">{</span><span class="n">y_train_wine</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Actual class test data point:      [2]
Predicted class: test data point:  [2]
Actual class first training point: 1
</pre></div>
</div>
</div>
</div>
<p>Now let us create a combination between the covariate values for the class 2 and class 1 predicted data points (for the sake of argument let’s ignore wheter this is a meaningful operation or not).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X_train_wine</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X_test_wine</span>
</pre></div>
</div>
</div>
</div>
<p>We can again form a predictive distributon and obtain the following:</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">bnn_wine</span><span class="p">:</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">set_data</span><span class="p">({</span><span class="s2">&quot;ann_input&quot;</span><span class="p">:</span> <span class="n">test</span><span class="p">,</span> <span class="s2">&quot;ann_output&quot;</span><span class="p">:</span> <span class="n">y_test_wine</span><span class="p">},</span>
                <span class="n">coords</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;obs_id&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X_test_wine</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])})</span>
    <span class="n">test_data</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide.input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">plot_dist</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s2">&quot;out&quot;</span><span class="p">],</span> <span class="n">kind</span> <span class="o">=</span> <span class="s2">&quot;hist&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Samples from Predictive Distribution&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;relative Frequency&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/6ae775edf717278adc36e6663a2986ee8fab4273a56a9ec5dea16e86342801ff.png" src="../_images/6ae775edf717278adc36e6663a2986ee8fab4273a56a9ec5dea16e86342801ff.png" />
</div>
</div>
<p>Still the correct prediction in terms of maximum predictive aposteriori mode is class 2 but we can observe that roughly 30% of our sampled values would opt for class 1! Actually a tiny franction of samples from our weights also resulted in a Class 0 prediction (otherwise there would be no block in the picture).</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">bnn_wine</span><span class="p">:</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">set_data</span><span class="p">({</span><span class="s2">&quot;ann_input&quot;</span><span class="p">:</span> <span class="n">test</span><span class="p">,</span> <span class="s2">&quot;ann_output&quot;</span><span class="p">:</span> <span class="n">y_test_wine</span><span class="p">},</span>
                <span class="n">coords</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;obs_id&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X_test_wine</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])})</span>
    <span class="n">test_data</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="variational-methods">
<h2><span class="section-number">12.3. </span>Variational Methods<a class="headerlink" href="#variational-methods" title="Link to this heading">#</a></h2>
<p>A nice property of MCMC methods is that asymptotically we get valid posterior distributions. The price we pay is time and computational burdens since we may need a long “burn-in” period and multiple chains are typically used to assess convergence.
A possible solution is to dismiss asymptocically correct results and live with an “tolerable” approximation <span class="math notranslate nohighlight">\(q_{\phi}\)</span> of the true posterior distribution which typically depends on some parameters <span class="math notranslate nohighlight">\(\phi\)</span>. Ideally we want to have <span class="math notranslate nohighlight">\(q_{\phi}({\bf w})\)</span> close to <span class="math notranslate nohighlight">\(p({\bf w} \vert {\mathcal{D}})\)</span>, therefore one needs to assess the discrepancy between distributions. One often used distance measure between two distribution <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> is the Kullback-Leibler Divergence:</p>
<div class="math notranslate nohighlight">
\[KL(f \vert \vert g) = \mathbb{E}_{f} \left[log \left( \frac{f({\bf x})}{g({\bf x})} \right) \right] =
    \int_{\mathcal{X}} f({\bf x}) \times log \left( \frac{f({\bf x})}{g({\bf x})} \right) d {\bf x}\]</div>
<p>Where expectation is taken with respect to the “target” distribution <span class="math notranslate nohighlight">\(f\)</span>. Notably the Kullback-Leibler Divergence is non-negative, takes a value of zero if <span class="math notranslate nohighlight">\(f\)</span> equals <span class="math notranslate nohighlight">\(g\)</span> and is not symmetric (and therefore does not fulfill the definition of a metric).
If the support of the variational distribution does not lie within the support of the posterior distribution one can see that <span class="math notranslate nohighlight">\(KL(f \vert \vert g) = \infty\)</span>. Therefore minimizing KL divergence needs to take this into account.</p>
<p>Now Consider the following functional:
$<span class="math notranslate nohighlight">\( t(q_{\phi}) = \int_{\mathcal{W}} q_{\phi} \times log \left( \frac{q_{\phi}({\bf w})}{p( {\bf w}, \mathcal{D})} \right) d {\bf w}\)</span>$
Rewriting we get:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \\ \int q_{\phi}({\bf w}) \times log \left( \frac{q_{\phi}({\bf w})}{p( {\bf w}, \mathcal{D})} \right) d {\bf w} 
     &amp; = \int q_{\phi}({\bf w}) \times log \left( \frac{q_{\phi}({\bf w})}{p({\bf w} \vert \mathcal{D}) \times p(\mathcal{D})} \right) d {\bf w} 
    \\ &amp; = \int q_{\phi}({\bf w}) \times \left[ log \left( \frac{q_{\phi}({\bf w})}{p({\bf w} \vert \mathcal{D})}\right) - log (p(\mathcal{D})) \right]d {\bf w} 
    \\ &amp; = \int q_{\phi}({\bf w}) \times log \left( \frac{q_{\phi}({\bf w})}{p({\bf w} \vert \mathcal{D})} \right) d {\bf w}  -log(p(\mathcal{D})) \underbrace{\int q_{\phi}({\bf w}) d {\bf w}}_{=1}
    \\ &amp; = KL(q_{\phi} \vert \vert p) - log(\underbrace{p(\mathcal{D})}_{\text{evidence}})
\end{align*}\]</div>
<p>Where we used the fact that densities integrate to one  next to the product rule for probabilities as well as logarithm rules.
One can see that the  negative log-evedince term is an upper bound on the functional <span class="math notranslate nohighlight">\(t(q_{\phi})\)</span> since <span class="math notranslate nohighlight">\(KL(q_{\phi} \vert \vert p) \geq 0\)</span>  and
<span class="math notranslate nohighlight">\(t(q_{\phi}) = KL(q_{\phi} \vert \vert p) - log(p(\mathcal{D})) \Rightarrow ELBO \leq  log(p(\mathcal{D}))\)</span> where ELBO is an abbreviation for <em>Evidence Lower Bound</em>, a common name for  <span class="math notranslate nohighlight">\(-t(q_{\phi})\)</span> .
Therefore instead of minimizing Kullback-Leibler Divergence one can also maximize <span class="math notranslate nohighlight">\(ELBO\)</span> (as <span class="math notranslate nohighlight">\(p(\mathcal{D})\)</span> does not vary for given data).</p>
<p>Now let us reset the model to the original training data and fit an approximation of the actual distributon.<br>
By default PyMc uses a mean field approximation, a mixture of gaussian distributions.</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">bnn_wine</span><span class="p">:</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">set_data</span><span class="p">({</span><span class="s2">&quot;ann_input&quot;</span><span class="p">:</span> <span class="n">X_train_wine</span><span class="p">,</span> <span class="s2">&quot;ann_output&quot;</span><span class="p">:</span> <span class="n">y_train_wine</span><span class="p">},</span>
                <span class="n">coords</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;obs_id&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X_train_wine</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])})</span>
    <span class="n">approx</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">n</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To check, whether we approximated the original target let us plot the approimation loss in terms of negative ELBO.
Since computing the gradient for the whole dataset doesn’t scale well and is computational demanding, by default the gradient in each epoch is sampled once to approximate the gradient of the whole dataset (therefore the loss does vizually seem to be more noisy).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">approx</span><span class="o">.</span><span class="n">hist</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Approximation Loss (ELBO)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;-ELBO&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/204cabf428ca4098ccafa11928abf384ba2b6d83c1ad913837ed1c86c78f45f2.png" src="../_images/204cabf428ca4098ccafa11928abf384ba2b6d83c1ad913837ed1c86c78f45f2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">bnn_wine</span><span class="p">:</span>
    <span class="n">new_trace</span> <span class="o">=</span> <span class="n">approx</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">draws</span> <span class="o">=</span> <span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now a small check whether we captured the original data distribution, just as above:</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">bnn_wine</span><span class="p">:</span>
    <span class="n">ppc_new</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">new_trace</span><span class="p">)</span>
    <span class="n">new_trace</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">ppc</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_dist</span><span class="p">(</span><span class="n">new_trace</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s2">&quot;out&quot;</span><span class="p">],</span> <span class="n">kind</span> <span class="o">=</span> <span class="s2">&quot;hist&quot;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Posterior Predictive distribution&quot;</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_train_wine</span><span class="p">,</span> <span class="n">return_counts</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">counts</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Actual data distribution&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/09830f50fd03eb9c3aecf91bb4c6570bfe8efd46085094d50be98d85df2a1e5a.png" src="../_images/09830f50fd03eb9c3aecf91bb4c6570bfe8efd46085094d50be98d85df2a1e5a.png" />
</div>
</div>
<p>Again we update the model with new data an create an artifical test case:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X_train_wine</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">X_test_wine</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">bnn_wine</span><span class="p">:</span>
    <span class="n">pm</span><span class="o">.</span><span class="n">set_data</span><span class="p">({</span><span class="s2">&quot;ann_input&quot;</span><span class="p">:</span> <span class="n">test</span><span class="p">,</span> <span class="s2">&quot;ann_output&quot;</span><span class="p">:</span> <span class="n">y_test_wine</span><span class="p">},</span>
                <span class="n">coords</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;obs_id&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X_test_wine</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])})</span>
    <span class="n">new_test_data</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">new_trace</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">plot_dist</span><span class="p">(</span><span class="n">new_test_data</span><span class="o">.</span><span class="n">posterior_predictive</span><span class="p">[</span><span class="s2">&quot;out&quot;</span><span class="p">],</span> <span class="n">kind</span> <span class="o">=</span> <span class="s2">&quot;hist&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Samples from Predictive Distribution&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Class&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;relative Frequency&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/27a1c4272a115e85060253f9e67995fa1973638c0f4d15fa363aef5ccc42ba9b.png" src="../_images/27a1c4272a115e85060253f9e67995fa1973638c0f4d15fa363aef5ccc42ba9b.png" />
</div>
</div>
<p>The single best predicted class would still be class 2 but observe that we are facing much higher epistemic uncertainty. Class 0 and class 1 are not very unlikely anymore. In fact one could argue that no class is an obvious choice at all since all have roughly same estimated posterior predictive probability.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter-bayesian_neuralnetwork"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter-deep_neuralnetwork/dnn.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11. </span>Deep Neural Network Ensembles</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter-credal_sets/credal_sets.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>Credal Sets and Classifiers</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">12.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-monte-carlo-methods">12.2. Markov Chain Monte Carlo Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-methods">12.3. Variational Methods</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <section>
    For comprehensive discussions please contact one of our team members: 
    {Evert.Buzon,Jiawen.Wang,Nico.Ploehn,S.Thies,Sven.Morlock,Zuo.Longfei}@campus.lmu.de

        <div style="margin-top: 50px;" id="disqus_thread"></div>

        <script>
            (function() { 
                var d = document, s = d.createElement('script');
                s.src = 'https://https-werywjw-github-io-toolbox-github-io.disqus.com/embed.js';  
                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
        })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </section> 

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>