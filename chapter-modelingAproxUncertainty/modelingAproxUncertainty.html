
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. Modelling Approximation Uncertainty &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/myStyle.css?v=e90502a2" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"N": ["\\mathbb{N}"], "vec": ["\\boldsymbol{#1}", 1], "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"], "defeq": ["\\mathrel{\\vcenter{\\baselineskip0.5ex \\lineskiplimit0pt \\hbox{\\footnotesize.}\\hbox{\\footnotesize.}}}% ="], "given": ["\\, | \\,"], "cX": ["\\mathcal{X}"], "cY": ["\\mathcal{Y}"], "cH": ["\\mathcal{H}"], "cD": ["\\mathcal{D}"], "hath": ["\\hat{h}"], "haty": ["\\hat{y}"], "hatp": ["\\hat{p}"], "sety": ["\\widehat{Y}"], "argmin": ["\\operatorname*{argmin}"], "argmax": ["\\operatorname*{argmax}"], "db": ["\\set{M}"], "fkt": ["#1(\\cdot)", 1], "chrfkt": ["\\mathbb{I}_{#1}", 1], "kref": ["(\\ref{#1})", 1], "convto": ["(\\rightarrow"], "fft": ["(#1 :  #2 \\rightarrow #3", 3], "with": ["\\,  | \\,"], "sothat": ["\\,  : \\,"], "defi": ["\\stackrel{\\on{df}}{=}"], "set": ["\\mathcal{#1}", 1], "Prob": ["P"], "prob": ["p"], "impl": ["\\Rightarrow"], "on": ["\\operatorname"], "groesser": ["\\raisebox{#1mm}{} \\raisebox{-#1mm}{}", 1], "sgroesser": ["\\groesser{1.20}"], "xleftr": ["\\left( \\groesser{1.35} "], "xleftg": ["\\left\\{ \\groesser{1.35} "], "fftm": ["\\fft{#1}{#2}{#3} \\, ,\\, #4 \\mapsto #5", 5], "gdw": ["\\Leftrightarrow"], "gdwbd": ["\\stackrel{\\on{df}}{\\Leftrightarrow}"], "est": ["{est}"], "epd": ["\\Leftrightarrow_{\\on{def}}"], "fromto": ["\\longrightarrow"], "pref": ["\\succ"], "evalue": ["\\mathbf{E}"], "variance": ["\\mathbf{V}"], "mmp": ["", 1], "llbracket": ["\\lbrack\\lbrack"], "rrbracket": ["\\rbrack\\rbrack"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter-modelingAproxUncertainty/modelingAproxUncertainty';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Probability Estimation via Scoring" href="../chapter-pe-scoring/scoring.html" />
    <link rel="prev" title="3. Sources of uncertainty in supervised learning" href="../chapter-srcUncertainty/src.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../startPage.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../startPage.html">
                    Toolbox for Uncertainty Quantification in Machine Learning
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../chapter-prelude/prelude.html">1. Prelude</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-intro/intro.html">2. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-srcUncertainty/src.html">3. Sources of uncertainty in supervised learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. Modelling Approximation Uncertainty</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-pe-scoring/scoring.html">5. Probability Estimation via Scoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-pe-calibration/calibration.html">6. Probability Estimation and Calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-pe-ensemble/ensemble.html">7. Probability Estimation and Ensembles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-mle_and_fisher/mle.html">8. Maximum Likelihood and Fisher Information</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-generative_models/gm.html">9. Generative Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-gaussianprocess/gaussianprocess.html">10. Gaussian Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-deep_neuralnetwork/dnn.html">11. Deep Neural Network Ensembles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-bayesian_neuralnetwork/bayesian.html">12. Bayesian Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-credal_sets/credal_sets.html">13. Credal Sets and Classifiers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-reliable_classification/reliable_classification.html">14. Reliable Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-conformel_classification/conformel_classification.html">15. Conformal Prediction for Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-conformel_regression/conformel_regression.html">16. Conformal Prediction for Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-setValued_utilityMaximization/set.html">17. Set-valued Prediction Based on Utility Maximization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter-appendix/appendix.html">18. Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">19. References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/Advueu963/PageTest/blob/main/chapter-modelingAproxUncertainty/modelingAproxUncertainty.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/Advueu963/PageTest" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Advueu963/PageTest/edit/main/chapter-modelingAproxUncertainty/modelingAproxUncertainty.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/Advueu963/PageTest/issues/new?title=Issue%20on%20page%20%2Fchapter-modelingAproxUncertainty/modelingAproxUncertainty.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/chapter-modelingAproxUncertainty/modelingAproxUncertainty.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Modelling Approximation Uncertainty</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#version-space-learning">4.1. Version Space Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-inference">4.2. Bayesian Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-a-lack-of-knowledge">4.3. Representing a lack of knowledge</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="modelling-approximation-uncertainty">
<span id="sbvd"></span><h1><span class="section-number">4. </span>Modelling Approximation Uncertainty<a class="headerlink" href="#modelling-approximation-uncertainty" title="Link to this heading">#</a></h1>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pymc</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>
<span class="kn">import</span> <span class="nn">xarray</span> <span class="k">as</span> <span class="nn">xr</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Vector Graphics</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib_inline</span>
<span class="n">matplotlib_inline</span><span class="o">.</span><span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="s1">&#39;svg&#39;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">matplotlib.cm</span> <span class="k">as</span> <span class="nn">cm</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span><span class="p">,</span> <span class="n">combinations</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
</pre></div>
</div>
</div>
</div>
<div align="justify">
<p>Bayesian inference can be seen as the main representative of probabilistic methods and provides a coherent framework for statistical reasoning that is well-established in machine learning (and beyond). Version space learning can be seen as a “logical” (and in a sense simplified) counterpart of Bayesian inference, in which hypotheses and predictions are not assessed numerically in terms of probabilities, but only qualified (deterministically) as being possible or impossible. In spite of its limited practical usefulness, version space learning is interesting for various reasons. In particular, in light of our discussion about uncertainty, it constitutes an interesting case: By construction, version space learning is free of aleatoric uncertainty, i.e., all uncertainty is epistemic.</p>
</div><section id="version-space-learning">
<span id="vsl"></span><h2><span class="section-number">4.1. </span>Version Space Learning<a class="headerlink" href="#version-space-learning" title="Link to this heading">#</a></h2>
<div align="justify">
<p>In the idealized setting of version space learning, we assume a deterministic dependency <span class="math notranslate nohighlight">\(f^*:\, \cX \longrightarrow \cY\)</span>,  i.e., the distribution <a class="reference internal" href="../chapter-srcUncertainty/src.html#equation-ccp">(3.5)</a> degenerates to</p>
<div class="math notranslate nohighlight" id="equation-ccpvs">
<span class="eqno">(4.1)<a class="headerlink" href="#equation-ccpvs" title="Link to this equation">#</a></span>\[\begin{split}
P( y \vert \vec{x}_{q}) = \left\{ \begin{array}{ll}
1 &amp; \text{ if } y = f^*(\vec{x}_{q}) \\
0 &amp; \text{ if } y \neq f^*(\vec{x}_{q}) \\
\end{array} \right.
\end{split}\]</div>
<p>Moreover, the training data <a class="reference internal" href="../chapter-srcUncertainty/src.html#equation-td">(3.1)</a> is free of noise. Correspondingly, we also assume that classifiers produce deterministic predictions <span class="math notranslate nohighlight">\(h(\vec{x}) \in \{ 0, 1 \}\)</span> in the form of probabilities 0 or 1. Finally, we assume that <span class="math notranslate nohighlight">\(f^* \in \cH\)</span>, and therefore <span class="math notranslate nohighlight">\(h^* = f^*\)</span> (which means there is no model uncertainty).</p>
</div><div align="justify">
<p>Under these assumptions, a hypothesis <span class="math notranslate nohighlight">\(h \in \cH\)</span> can be eliminated as a candidate as soon as it makes at least one mistake on the training data: in that case, the risk of <span class="math notranslate nohighlight">\(h\)</span> is necessarily higher than the risk of <span class="math notranslate nohighlight">\(h^*\)</span> (which is 0). The idea of the candidate elimination algorithm (<span id="id1">Mitchell [<a class="reference internal" href="../references.html#id1428" title="T.M. Mitchell. Version spaces: A candidate elimination approach to rule learning. In Proceedings \sc IJCAI-77, 305-310. 1977.">Mit77</a>]</span>) is to maintain the <em>version space</em> <span class="math notranslate nohighlight">\(\mathcal{V} \subseteq \cH\)</span> that consists of the set of all hypotheses consistent with the data seen so far:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V} = \mathcal{V}(\cH , \cD) = \{ h \in \cH \with h(\vec{x}_i) = y_i \text{ for } i = 1, \ldots , N \Big\}
\]</div>
<p>Obviously, the version space is shrinking with an increasing amount of training data, i.e., <span class="math notranslate nohighlight">\(\mathcal{V}(\cH , \cD') \subseteq \mathcal{V}(\cH , \cD)\)</span> for <span class="math notranslate nohighlight">\(\cD \subseteq \cD'\)</span>.</p>
</div><div align="justify">
<p>If a prediction <span class="math notranslate nohighlight">\(\hat{y}_{q}\)</span> for a query instance <span class="math notranslate nohighlight">\(\vec{x}_{q}\)</span> is sought, this query is submitted to all members <span class="math notranslate nohighlight">\(h \in \mathcal{V}\)</span> of the version space. Obviously, a unique prediction can only be made if all members agree on the outcome of <span class="math notranslate nohighlight">\(\vec{x}_{q}\)</span>. Otherwise, several outcomes <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span> may still appear possible. Formally, mimicking the logical conjunction with the minimum operator and the existential quantification with a maximum, we can express the degree of possibility or plausibility of an outcome <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span> as follows (<span class="math notranslate nohighlight">\(\llbracket \cdot \rrbracket\)</span> denotes the indicator function):</p>
<div class="math notranslate nohighlight" id="equation-ee1">
<span class="eqno">(4.2)<a class="headerlink" href="#equation-ee1" title="Link to this equation">#</a></span>\[
\pi(y) := \max_{h \in \cH} \min \Big( \llbracket h \in \mathcal{V} \rrbracket , \llbracket h(\vec{x}_q) = y \rrbracket \Big)
\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\pi(y)=1\)</span> if there exists a candidate hypothesis <span class="math notranslate nohighlight">\(h \in \mathcal{V}\)</span> such that <span class="math notranslate nohighlight">\(h(\vec{x}_{q}) = y\)</span>, and <span class="math notranslate nohighlight">\(\pi(y)=0\)</span> otherwise. In other words, the prediction produced in version space learning is a subset</p>
<div class="math notranslate nohighlight" id="equation-vss">
<span class="eqno">(4.3)<a class="headerlink" href="#equation-vss" title="Link to this equation">#</a></span>\[
Y = Y(\vec{x}_q) := \{ h(\vec{x}_q) \vert h \in \mathcal{V} \} = \{ y \vert \pi(y) = 1 \} \subseteq \mathcal{Y}
\]</div>
<p>See <a class="reference internal" href="#vs"><span class="std std-numref">Fig. 4.1</span></a> for an illustration.</p>
</div><figure class="align-default" id="vs">
<a class="reference internal image-reference" href="../_images/pic-version-space.png"><img alt="version-space" src="../_images/pic-version-space.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.1 </span><span class="caption-text">Illustration of version space learning inference. The version space <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>, which is the subset of hypotheses consistent with the data seen so far, represents the state of knowledge of the learner. For a query <span class="math notranslate nohighlight">\(\vec{x}_{q} \in \mathcal{X}\)</span>, the set of possible predictions is given by the set <span class="math notranslate nohighlight">\(Y = \{ h(\vec{x}_q) \given h \in \mathcal{V}\}\)</span>. The distribution on the right is the characteristic function <span class="math notranslate nohighlight">\(\pi:\, \mathcal{Y} \longrightarrow \{ 0,1 \}\)</span> of this set, which can be interpreted as a <span class="math notranslate nohighlight">\(\{0,1\}\)</span>-valued possibility distribution (indicating whether <span class="math notranslate nohighlight">\(y\)</span> is a possible outcome or not, i.e., <span class="math notranslate nohighlight">\(\pi(y) = \llbracket y \in Y \rrbracket\)</span>).</span><a class="headerlink" href="#vs" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div align="justify">
<p>Here’s an example about how to use the version space algorithm to classify weather data. We will start by generating and visualizing a dataset, define and update the version space, and finally use the version space to make predictions.</p>
</div><div align="justify">
<p>First, we generate some data with temperature and humidity as features, and weather type (sunny or rainy) as labels.</p>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">70</span><span class="p">],</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">65</span><span class="p">],</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">80</span><span class="p">],</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">90</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">85</span><span class="p">],</span>  <span class="c1"># Features (e.g., temperature, humidity)</span>
    <span class="p">[</span><span class="mi">35</span><span class="p">,</span> <span class="mi">60</span><span class="p">],</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">55</span><span class="p">],</span> <span class="p">[</span><span class="mi">45</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">45</span><span class="p">],</span> <span class="p">[</span><span class="mi">55</span><span class="p">,</span> <span class="mi">40</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># Labels (e.g., sunny=1, rainy=0)</span>

<span class="n">training_data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">scatter</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Training Data&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Temperature&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Humidity&#39;</span><span class="p">)</span>

<span class="n">legend_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Rainy&#39;</span><span class="p">,</span> <span class="s1">&#39;Sunny&#39;</span><span class="p">]</span>
<span class="n">handles</span> <span class="o">=</span> <span class="n">scatter</span><span class="o">.</span><span class="n">legend_elements</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="p">,</span> <span class="n">legend_labels</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Weather Type&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/054c46d3247d20ebaacea1ac6953e1bc6f84901570bd407ad1321388d58a0173.svg" src="../_images/054c46d3247d20ebaacea1ac6953e1bc6f84901570bd407ad1321388d58a0173.svg" />
</div>
</div>
<div align="justify">
<p>We also define the initial version space, which contains several possible rules. Each rule in the version space is represented by a range of temperature and humidity. A range acts as a boundary distinguishing between positive examples (sunny days) and negative examples (rainy days).</p>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initial version space</span>
<span class="n">version_space</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">&#39;temp_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">35</span><span class="p">),</span> <span class="s1">&#39;humidity_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">60</span><span class="p">,</span> <span class="mi">80</span><span class="p">)},</span>
    <span class="p">{</span><span class="s1">&#39;temp_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="s1">&#39;humidity_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">75</span><span class="p">,</span> <span class="mi">100</span><span class="p">)},</span>
    <span class="p">{</span><span class="s1">&#39;temp_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="s1">&#39;humidity_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">70</span><span class="p">)},</span>
    <span class="p">{</span><span class="s1">&#39;temp_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="s1">&#39;humidity_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">70</span><span class="p">,</span> <span class="mi">100</span><span class="p">)},</span>
    <span class="p">{</span><span class="s1">&#39;temp_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="s1">&#39;humidity_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">65</span><span class="p">)},</span>
    <span class="p">{</span><span class="s1">&#39;temp_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="s1">&#39;humidity_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">65</span><span class="p">,</span> <span class="mi">100</span><span class="p">)},</span>
    <span class="p">{</span><span class="s1">&#39;temp_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">60</span><span class="p">),</span> <span class="s1">&#39;humidity_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">45</span><span class="p">,</span> <span class="mi">75</span><span class="p">)},</span>
    <span class="p">{</span><span class="s1">&#39;temp_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">45</span><span class="p">),</span> <span class="s1">&#39;humidity_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">55</span><span class="p">,</span> <span class="mi">90</span><span class="p">)},</span>
    <span class="p">{</span><span class="s1">&#39;temp_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">40</span><span class="p">),</span> <span class="s1">&#39;humidity_range&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">70</span><span class="p">,</span> <span class="mi">100</span><span class="p">)}</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div align="justify">
<p>We now define a function to update the version space based on the training data. The function removes rules that are inconsistent with the training examples. Suppose through a series of training data, which includes both sunny and rainy conditions, the version space algorithm ultimately narrows down to a specific set of rules. These rules may form boundaries around specific combinations of temperature and humidity.</p>
<p>The final version space should:</p>
<ol class="arabic simple">
<li><p>Cover all positives: The rules within the final version space must cover all data points labeled as positive, ensuring that these points are correctly classified as positive examples.</p></li>
<li><p>Exclude all negatives: The rules within the final version space should not encompass any data points labeled as negative, ensuring that these points are correctly classified as negative examples.</p></li>
</ol>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_version_space</span><span class="p">(</span><span class="n">version_space</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">updated_space</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">rule</span> <span class="ow">in</span> <span class="n">version_space</span><span class="p">:</span>
        <span class="n">temp_min</span><span class="p">,</span> <span class="n">temp_max</span> <span class="o">=</span> <span class="n">rule</span><span class="p">[</span><span class="s1">&#39;temp_range&#39;</span><span class="p">]</span>
        <span class="n">humidity_min</span><span class="p">,</span> <span class="n">humidity_max</span> <span class="o">=</span> <span class="n">rule</span><span class="p">[</span><span class="s1">&#39;humidity_range&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  
            <span class="k">if</span> <span class="n">temp_min</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">temp_max</span> <span class="ow">and</span> <span class="n">humidity_min</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">humidity_max</span><span class="p">:</span>
                <span class="n">updated_space</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rule</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span> 
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">temp_min</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">temp_max</span> <span class="ow">and</span> <span class="n">humidity_min</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">humidity_max</span><span class="p">):</span>
                <span class="n">updated_space</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rule</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">updated_space</span>

<span class="n">current_version_space</span> <span class="o">=</span> <span class="n">version_space</span>

<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">:</span>
    <span class="n">current_version_space</span> <span class="o">=</span> <span class="n">update_version_space</span><span class="p">(</span><span class="n">current_version_space</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">current_version_space</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No hypotheses left in the version space.&quot;</span><span class="p">)</span>
        <span class="k">break</span>
</pre></div>
</div>
</div>
</div>
<div align="justify">
<p>Now we visualize the initial and updated version space using rectangles to represent the ranges of temperature and humidity. Each rectangle represents a rule that defines a range for temperature and humidity:</p>
<p>Horizontal Axis: Represents the range of temperature. The horizontal width of the rectangle extends from the minimum temperature value to the maximum temperature value.</p>
<p>Vertical Axis: Represents the range of humidity. The vertical height of the rectangle extends from the minimum humidity value to the maximum humidity value.</p>
</div><div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>

<span class="n">colors</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">rainbow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">version_space</span><span class="p">)))</span>
<span class="n">rule_to_color</span> <span class="o">=</span> <span class="p">{</span><span class="nb">str</span><span class="p">(</span><span class="n">rule</span><span class="p">):</span> <span class="n">color</span> <span class="k">for</span> <span class="n">rule</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">version_space</span><span class="p">,</span> <span class="n">colors</span><span class="p">)}</span>

<span class="k">for</span> <span class="n">rule</span> <span class="ow">in</span> <span class="n">version_space</span><span class="p">:</span>
    <span class="n">temp_min</span><span class="p">,</span> <span class="n">temp_max</span> <span class="o">=</span> <span class="n">rule</span><span class="p">[</span><span class="s1">&#39;temp_range&#39;</span><span class="p">]</span>
    <span class="n">humidity_min</span><span class="p">,</span> <span class="n">humidity_max</span> <span class="o">=</span> <span class="n">rule</span><span class="p">[</span><span class="s1">&#39;humidity_range&#39;</span><span class="p">]</span>
    <span class="n">color</span> <span class="o">=</span> <span class="n">rule_to_color</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">rule</span><span class="p">)]</span>
    <span class="n">rect</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">temp_min</span><span class="p">,</span> <span class="n">humidity_min</span><span class="p">),</span> <span class="n">temp_max</span><span class="o">-</span><span class="n">temp_min</span><span class="p">,</span> <span class="n">humidity_max</span><span class="o">-</span><span class="n">humidity_min</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Initial Version Space&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Temperature&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Humidity&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="k">for</span> <span class="n">rule</span> <span class="ow">in</span> <span class="n">current_version_space</span><span class="p">:</span>
    <span class="n">temp_min</span><span class="p">,</span> <span class="n">temp_max</span> <span class="o">=</span> <span class="n">rule</span><span class="p">[</span><span class="s1">&#39;temp_range&#39;</span><span class="p">]</span>
    <span class="n">humidity_min</span><span class="p">,</span> <span class="n">humidity_max</span> <span class="o">=</span> <span class="n">rule</span><span class="p">[</span><span class="s1">&#39;humidity_range&#39;</span><span class="p">]</span>
    <span class="n">color</span> <span class="o">=</span> <span class="n">rule_to_color</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">rule</span><span class="p">)]</span>
    <span class="n">rect</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">temp_min</span><span class="p">,</span> <span class="n">humidity_min</span><span class="p">),</span> <span class="n">temp_max</span><span class="o">-</span><span class="n">temp_min</span><span class="p">,</span> <span class="n">humidity_max</span><span class="o">-</span><span class="n">humidity_min</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Updated Version Space&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Temperature&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Humidity&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">60</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;After updating, the final version space contains:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">current_version_space</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>After updating, the final version space contains:
[{&#39;temp_range&#39;: (25, 50), &#39;humidity_range&#39;: (0, 70)}, {&#39;temp_range&#39;: (20, 60), &#39;humidity_range&#39;: (45, 75)}]
</pre></div>
</div>
<img alt="../_images/6c2bfadfaa44322f8fe37a8fbef8af39e41933e950c1aab70884ea3af9b73fe1.svg" src="../_images/6c2bfadfaa44322f8fe37a8fbef8af39e41933e950c1aab70884ea3af9b73fe1.svg" />
</div>
</div>
<div align="justify">
<p>Finally, we define a function to make predictions based on the updated final version space and test it with a new sample(temperature=25, humidity=68). The “predict” function iterates over each rule in the final version space and checks if the new sample falls within the temperature and humidity range specified by the rule. The function returns the list of predictions, “0” for “Rainy” and “1” for “Sunny”. We check the predictions list to see if 0 or 1 is present. If present, we set the corresponding possibility to 1, otherwise to 0.</p>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">version_space</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">version_space</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>  
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">rule</span> <span class="ow">in</span> <span class="n">version_space</span><span class="p">:</span>
        <span class="n">temp_min</span><span class="p">,</span> <span class="n">temp_max</span> <span class="o">=</span> <span class="n">rule</span><span class="p">[</span><span class="s1">&#39;temp_range&#39;</span><span class="p">]</span>
        <span class="n">humidity_min</span><span class="p">,</span> <span class="n">humidity_max</span> <span class="o">=</span> <span class="n">rule</span><span class="p">[</span><span class="s1">&#39;humidity_range&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">temp_min</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">temp_max</span> <span class="ow">and</span> <span class="n">humidity_min</span> <span class="o">&lt;=</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">humidity_max</span><span class="p">:</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">predictions</span>  

<span class="c1"># New sample</span>
<span class="n">new_sample</span> <span class="o">=</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">68</span><span class="p">]</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">current_version_space</span><span class="p">,</span> <span class="n">new_sample</span><span class="p">)</span>

<span class="c1"># Representing the possibility of outcomes</span>
<span class="n">outcomes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Rainy&quot;</span><span class="p">,</span> <span class="s2">&quot;Sunny&quot;</span><span class="p">]</span>
<span class="k">if</span> <span class="n">predictions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">outcome_possibility</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">outcome_possibility</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">predictions</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">bars</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">outcomes</span><span class="p">,</span> <span class="n">outcome_possibility</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;orange&#39;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Outcome Possibility&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">fontweight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Outcomes&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Possibility&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> 

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The possibility for two classes(rainy or sunny) is: </span><span class="si">{</span><span class="n">outcome_possibility</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The possibility for two classes(rainy or sunny) is: [0, 1]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<img alt="../_images/90c5c038c500aa822415e04af18b0bba9e0bb1296742c8ab7b5a2d793cc72776.svg" src="../_images/90c5c038c500aa822415e04af18b0bba9e0bb1296742c8ab7b5a2d793cc72776.svg" />
</div>
</div>
<div align="justify">
<p>Note that the inference <a class="reference internal" href="#equation-ee1">(4.2)</a> can be seen as a kind of constraint propagation, in which the constraint <span class="math notranslate nohighlight">\(h \in \mathcal{V}\)</span> on the hypothesis space <span class="math notranslate nohighlight">\(\cH\)</span> is propagated to a constraint on <span class="math notranslate nohighlight">\(\cY\)</span>, expressed in the form of the subset <a class="reference internal" href="#equation-vss">(4.3)</a> of possible outcomes; or symbolically:</p>
<div class="math notranslate nohighlight" id="equation-cbi">
<span class="eqno">(4.4)<a class="headerlink" href="#equation-cbi" title="Link to this equation">#</a></span>\[
\cH , \cD , \vec{x}_{q}  \models Y
\]</div>
<p>This view highlights the interaction between prior knowledge and data: It shows that what can be said about the possible outcomes <span class="math notranslate nohighlight">\(y_{q}\)</span> not only depends on the data <span class="math notranslate nohighlight">\(\cD\)</span> but also on the hypothesis space <span class="math notranslate nohighlight">\(\cH\)</span>, i.e., the <em>model assumptions</em> the learner starts with. The specification of <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> always comes with an <em>inductive bias</em>, which is indeed essential for learning from data <span id="id2">Mitchell [<a class="reference internal" href="../references.html#id1613" title="T.M. Mitchell. The need for biases in learning generalizations. Technical Report TR CBM–TR–117, Rutgers University, 1980.">Mit80</a>]</span>. In general, both aleatoric and epistemic uncertainty (ignorance) depend on the way in which prior knowledge and data interact with each other. Roughly speaking, the stronger the knowledge the learning process starts with, the less data is needed to resolve uncertainty. In the extreme case, the true model is already known, and data is completely superfluous. Normally, however, prior knowledge is specified by assuming a certain type of model, for example a linear relationship between inputs <span class="math notranslate nohighlight">\(\vec{x}\)</span> and outputs <span class="math notranslate nohighlight">\(y\)</span>. Then, all else (namely the data) being equal, the degree of predictive uncertainty depends on how flexible the corresponding model class is. Informally speaking, the more restrictive the model assumptions are, the smaller the uncertainty will be. This is illustrated in <a class="reference internal" href="#vsli"><span class="std std-numref">Fig. 4.2</span></a> for the case of binary classification.</p>
</div><figure class="align-default" id="vsli">
<a class="reference internal image-reference" href="../_images/combined_figure.png"><img alt="version-space" src="../_images/combined_figure.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.2 </span><span class="caption-text">Two examples illustrating predictive uncertainty in version space learning for the case of binary classification.
In the first example (above), the hypothesis space is given in terms of rectangles (assigning interior instances to the positive class and instances outside to the negative class). Both pictures show some of the (infinitely many) elements of the version space. Top left: Restricting <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> to axis-parallel rectangles, the first query point is necessarily positive, the second one necessarily negative, because these predictions are produced by all <span class="math notranslate nohighlight">\(h \in \mathcal{V}\)</span>. As opposed to this, both positive and negative predictions can be produced for the third query. Top right: Increasing flexibility (or weakening prior knowledge) by enriching the hypothesis space and also allowing for non-axis-parallel rectangles, none of the queries can be classified with certainty anymore. In the second example (below), hypotheses are linear and quadratic discriminant functions, respectively. Bottom left: If the hypothesis space is given by linear discriminant functions, all hypothesis consistent with the data will predict the query instance as positive. Bottom right: Enriching the hypothesis space with quadratic discriminants, the members of the version space will no longer vote unanimously: some of them predict the positive and others the negative class.</span><a class="headerlink" href="#vsli" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div align="justify">
<p>Coming back to our discussion about uncertainty, it is clear that version space learning as outlined above does not involve any kind of aleatoric uncertainty. Instead, the only source of uncertainty is a lack of knowledge about <span class="math notranslate nohighlight">\(h^*\)</span>, and hence of epistemic nature. On the model level, the amount of uncertainty is in direct correspondence with the size of the version space <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> and reduces with an increasing sample size. Likewise, the predictive uncertainty could be measured in terms of the size of the set <a class="reference internal" href="#equation-vss">(4.3)</a> of candidate outcomes. Obviously, this uncertainty may differ from instance to instance, or, stated differently, approximation uncertainty may translate into prediction uncertainty in different ways.</p>
<p>In version space learning, uncertainty is represented in a purely set-based manner: the version space <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> and prediction set <span class="math notranslate nohighlight">\(Y(x_q)\)</span> are subsets of <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>, respectively. In other words, hypotheses <span class="math notranslate nohighlight">\(h \in \mathcal{H}\)</span> and outcomes <span class="math notranslate nohighlight">\(y \in \mathcal{Y}\)</span> are only qualified in terms of being possible or not. In the following, we discuss the Bayesian approach, in which hypotheses and predictions are qualified more gradually in terms of probabilities.</p>
</div></section>
<section id="bayesian-inference">
<h2><span class="section-number">4.2. </span>Bayesian Inference<a class="headerlink" href="#bayesian-inference" title="Link to this heading">#</a></h2>
<div align="justify">
<p>Consider a hypothesis space <span class="math notranslate nohighlight">\(\cH\)</span> consisting of probabilistic predictors, that is, hypotheses <span class="math notranslate nohighlight">\(h\)</span> that deliver probabilistic predictions <span class="math notranslate nohighlight">\(p_h(y \given \vec{x}) = p(y \vert \vec{x}, h)\)</span> of outcomes <span class="math notranslate nohighlight">\(y\)</span> given an instance <span class="math notranslate nohighlight">\(\vec{x}\)</span>. In the Bayesian approach, <span class="math notranslate nohighlight">\(\cH\)</span> is supposed to be equipped with a prior distribution <span class="math notranslate nohighlight">\(p(\cdot)\)</span>, and learning essentially consists of replacing the prior by the posterior distribution:</p>
<div class="math notranslate nohighlight" id="equation-bpost">
<span class="eqno">(4.5)<a class="headerlink" href="#equation-bpost" title="Link to this equation">#</a></span>\[
p(h \vert \cD) \, = \frac{p(h) \cdot p(\cD \vert h)}{p(\cD)} \,  \propto \, p(h) \cdot p(\cD \vert h) \, ,
\]</div>
<p>where <span class="math notranslate nohighlight">\(p(\cD \vert h)\)</span> is the probability of the data given <span class="math notranslate nohighlight">\(h\)</span> (the likelihood of <span class="math notranslate nohighlight">\(h\)</span>).
Intuitively, <span class="math notranslate nohighlight">\(p( \cdot \vert \cD)\)</span> captures the learner’s state of knowledge, and hence its epistemic uncertainty: The more “peaked” this distribution, i.e., the more concentrated the probability mass in a small region in <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, the less uncertain the learner is. Just like the version space <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> in version space learning, the posterior distribution on <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> provides global (averaged/aggregated over the entire instance space) instead of local, <em>per-instance</em> information. For a given query instance <span class="math notranslate nohighlight">\(\vec{x}_{q}\)</span>, this information may translate into quite different representations of the uncertainty regarding the prediction <span class="math notranslate nohighlight">\(\haty_{q}\)</span> (cf.<a class="reference internal" href="#bayesian"><span class="std std-numref">Fig. 4.3</span></a>).</p>
</div><figure class="align-default" id="bayesian">
<a class="reference internal image-reference" href="../_images/pic-bayesian.png"><img alt="settings" src="../_images/pic-bayesian.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.3 </span><span class="caption-text">Illustration of Bayesian inference. Updating a prior distribution on <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> based on training data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, a posterior distribution <span class="math notranslate nohighlight">\(\prob(h \vert \mathcal{D})\)</span> is obtained, which represents the state of knowledge of the learner (middle). Given a query <span class="math notranslate nohighlight">\(\vec{x}_{q} \in \mathcal{X}\)</span>, the predictive posterior distribution on <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is then obtained through Bayesian model averaging: Different hypotheses <span class="math notranslate nohighlight">\(h \in \mathcal{H}\)</span> provide predictions, which are aggregated in terms of a weighted average.</span><a class="headerlink" href="#bayesian" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Here’s a Python script demonstrating the application of Gaussian Naive Bayes (GaussianNB) for classification using synthetic data. Gaussian Naive Bayes is a simple instance of applying Bayesian inference, which assumes that features follow a Gaussian distribution. The script trains the model on a dataset and visualizes the training data distribution, the model’s decision boundaries, and the predicted probabilities for a test sample. These visualizations highlight how the Gaussian Naive Bayes model uses Bayesian principles to update beliefs and make predictions based on new data.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">gnb</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">gnb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_probs</span> <span class="o">=</span> <span class="n">gnb</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="s1">&#39;tab:green&#39;</span><span class="p">,</span> <span class="s1">&#39;tab:purple&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colors</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">i</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">i</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Class </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Training Data D&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 2&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">gnb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span> 
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">colors</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">i</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">i</span><span class="p">][:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Hypothesis Space H&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 2&quot;</span><span class="p">)</span>

<span class="n">outcomes</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;y</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">y_probs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">outcomes</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span> <span class="c1"># tab:blue</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Possibility of Outcomes&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Probability&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/db0ef872e1d0f1349fe61280d0c5780e3dc47cd2e47360e69adbd807272522b8.svg" src="../_images/db0ef872e1d0f1349fe61280d0c5780e3dc47cd2e47360e69adbd807272522b8.svg" />
</div>
</div>
<div align="justify">
<p>More specifically, the representation of uncertainty about a prediction <span class="math notranslate nohighlight">\(\haty_{q}\)</span> is given by the image of the posterior <span class="math notranslate nohighlight">\(p( h \vert \cD)\)</span> under the mapping <span class="math notranslate nohighlight">\(h \mapsto p(y \vert \vec{x}_{q}, h)\)</span> from hypotheses to probabilities of outcomes. This yields the predictive posterior distribution</p>
<div class="math notranslate nohighlight" id="equation-pd">
<span class="eqno">(4.6)<a class="headerlink" href="#equation-pd" title="Link to this equation">#</a></span>\[
p(y \vert \vec{x}_{q}) = 
\int_\cH p(y \vert \vec{x}_{q} , h) \, d\, p(h \vert \set{D}) \enspace .
\]</div>
<p>In this type of (proper) Bayesian inference, a final prediction is thus produced by <em>model averaging</em>: The predicted probability of an outcome <span class="math notranslate nohighlight">\(y \in \cY\)</span> is the <em>expected</em> probability <span class="math notranslate nohighlight">\(p(y \vert \vec{x}_{q}, h)\)</span>, where the expectation over the hypotheses is taken with respect to the posterior distribution on <span class="math notranslate nohighlight">\(\cH\)</span>; or, stated differently, the probability of an outcome is a weighted average over its probabilities under all hypotheses in <span class="math notranslate nohighlight">\(\cH\)</span>, with the weight of each hypothesis <span class="math notranslate nohighlight">\(h\)</span> given by its posterior probability <span class="math notranslate nohighlight">\(p( h \vert \cD)\)</span>.
Since model averaging is often difficult and computationally costly, sometimes only the single hypothesis</p>
<div class="math notranslate nohighlight" id="equation-map">
<span class="eqno">(4.7)<a class="headerlink" href="#equation-map" title="Link to this equation">#</a></span>\[
h^{map} := \argmax_{h \in \cH} p( h \vert \cD) 
\]</div>
<p>with the highest posterior probability is adopted, and predictions on outcomes are derived from this hypothesis.</p>
</div><div align="justify">
<p>To further understand how Bayesian inference can be applied, let’s see a practical example that utilizes advanced probabilistic programming with <a class="reference external" href="https://www.pymc.io/welcome.html">PyMC</a> library for a more streamlined approach. The following section breaks down the process, from data generation to model estimation and analysis, highlighting the advantages of Bayesian methods in statistical modeling.<a class="footnote-reference brackets" href="#footnoteidentifier1" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p>
</div><div align="justify">
<p>First, we need to generate a set of simulated data to mimic the relationship between house area and selling price. This data will serve as the input for our model, helping us understand how changes in house size could potentially affect pricing.</p>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">8927</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>

<span class="c1"># Generating house area data from 50 to 250 square meters</span>
<span class="n">area</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span> 
<span class="c1"># Basic price as the intercept at $100,000</span>
<span class="n">intercept</span> <span class="o">=</span> <span class="mi">100</span>  
<span class="c1"># Price increase per square meter as the slope</span>
<span class="n">slope</span> <span class="o">=</span> <span class="mi">2</span> 
<span class="c1"># Adding normally distributed noise to simulate real-world data variability</span>
<span class="n">noise</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span> 

<span class="n">true_line</span> <span class="o">=</span> <span class="n">intercept</span> <span class="o">+</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">area</span>

<span class="n">price</span> <span class="o">=</span> <span class="n">intercept</span> <span class="o">+</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">area</span> <span class="o">+</span> <span class="n">noise</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;area&#39;</span><span class="p">:</span> <span class="n">area</span><span class="p">,</span> <span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="n">price</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Area (sq meters)&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Price (thousand dollars)&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;House Prices vs. Area&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">area</span><span class="p">,</span> <span class="n">price</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;sampled data&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">area</span><span class="p">,</span> <span class="n">true_line</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;price line&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/3e735634acdbd9d999d576fdcc4d86ebe2955a321e5534a01e2afc75ba73984f.svg" src="../_images/3e735634acdbd9d999d576fdcc4d86ebe2955a321e5534a01e2afc75ba73984f.svg" />
</div>
</div>
<div align="justify">
<p>Using PyMC, we define a Bayesian linear regression model. We set priors for the intercept, slope, and the noise level, and then use these priors along with the observed data to estimate the posterior distributions. To infer the posterior distributions of the model parameters, 1000 samples are drawn using the NUTS (No-U-Turn Sampler) algorithm. NUTS is an advanced MCMC (<a class="reference internal" href="../chapter-bayesian_neuralnetwork/bayesian.html#mcmc"><span class="std std-ref">Markov Chain Monte Carlo</span></a>) sampling technique that efficiently handles the complexities of continuous parameter spaces and dependencies among parameters.</p>
</div><div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># Priors for unknown model parameters</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;Intercept&#39;</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">slope</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;Slope&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Linear model</span>
    <span class="n">price_est</span> <span class="o">=</span> <span class="n">intercept</span> <span class="o">+</span> <span class="n">slope</span> <span class="o">*</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;area&#39;</span><span class="p">]</span>
    <span class="c1"># Likelihood (sampling distribution) of observations</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">price_est</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">])</span>

    <span class="c1"># Drawing 1000 posterior samples using NUTS sampler</span>
    <span class="n">trace</span> <span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div align="justify">
<p>After sampling, it’s useful to visualize the posterior distributions of the model parameters to assess their uncertainty and the convergence of the sampling process.</p>
</div><div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Intercept&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="s1">&#39;Slope&#39;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/379d4c8d69cb6c843a9df7814e8c6b62d8f1af4d4495a398108230ac766d6912.svg" src="../_images/379d4c8d69cb6c843a9df7814e8c6b62d8f1af4d4495a398108230ac766d6912.svg" />
</div>
</div>
<div align="justify">
<p>Finally, we generate posterior predictive checks to see how well our model predicts new data. This step is crucial for validating the model’s performance and understanding its predictive capabilities.
In this approach, instead of relying on a single best-fitting regression line, we consider multiple possible lines. This is visualized through a posterior predictive plot, which illustrates each potential regression line by using different sets of sampled parameters, intercepts and slopes, from the posterior distribution. We can create these lines manually by directly utilizing the posterior samples.</p>
</div><div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&quot;y_model&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&quot;Intercept&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">trace</span><span class="o">.</span><span class="n">posterior</span><span class="p">[</span><span class="s2">&quot;Slope&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">xr</span><span class="o">.</span><span class="n">DataArray</span><span class="p">(</span><span class="n">area</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_lm</span><span class="p">(</span><span class="n">idata</span><span class="o">=</span><span class="n">trace</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;price&quot;</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">y_model</span><span class="o">=</span><span class="s2">&quot;y_model&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Posterior predictive regression lines&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Area (sq meters)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Price (thousand dollars)&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.12.4/x64/lib/python3.12/site-packages/arviz/plots/lmplot.py:212: UserWarning: posterior_predictive not found in idata
  warnings.warn(&quot;posterior_predictive not found in idata&quot;, UserWarning)
</pre></div>
</div>
<img alt="../_images/0d4631446f5d6f2a7291fc5938b5dfbaeeb4d1c38c2fd7a3fd528de8258a2a78.svg" src="../_images/0d4631446f5d6f2a7291fc5938b5dfbaeeb4d1c38c2fd7a3fd528de8258a2a78.svg" />
</div>
</div>
<div align="justify">
<p>In <a class="reference internal" href="#equation-pd">(4.6)</a>, aleatoric and epistemic uncertainty are not distinguished any more, because epistemic uncertainty is “averaged out.” Consider the example of coin flipping, and let the hypothesis space be given by <span class="math notranslate nohighlight">\(\mathcal{H} \{ h_\alpha \with 0 \leq \alpha \leq 1 \}\)</span>, where <span class="math notranslate nohighlight">\(h_\alpha\)</span> is modeling a biased coin landing heads up with a probability of <span class="math notranslate nohighlight">\(\alpha\)</span> and tails up with a probability of <span class="math notranslate nohighlight">\(1- \alpha\)</span>. According to <a class="reference internal" href="#equation-pd">(4.6)</a>, we derive a probability of <span class="math notranslate nohighlight">\(1/2\)</span> for heads and tails, regardless of whether the (posterior) distribution on <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is given by the uniform distribution (all coins are equally probable, i.e., the case of complete ignorance) or the one-point measure assigning probability 1 to <span class="math notranslate nohighlight">\(h_{1/2}\)</span> (the coin is known to be fair with complete certainty):</p>
<div class="math notranslate nohighlight">
\[
p(y ) = 
\int_\cH \alpha \, d\, p  = \frac{1}{2} = \int_\cH \alpha \, d\, p'  
\]</div>
<p>for the uniform measure <span class="math notranslate nohighlight">\(p\)</span> (with probability density function <span class="math notranslate nohighlight">\(p(\alpha) \equiv 1\)</span>) as well as the measure <span class="math notranslate nohighlight">\(p'\)</span> with probability mass function <span class="math notranslate nohighlight">\(p'(\alpha) = 1\)</span> if <span class="math notranslate nohighlight">\(\alpha = \frac{1}{2}\)</span> and <span class="math notranslate nohighlight">\(= 0\)</span> for <span class="math notranslate nohighlight">\(\alpha \neq \frac{1}{2}\)</span>.
Obviously, MAP inference <a class="reference internal" href="#equation-map">(4.7)</a> does not capture epistemic uncertainty either.</p>
</div><div align="justify">
<p>More generally, consider the case of binary classification with <span class="math notranslate nohighlight">\(\cY \{ -1, +1 \}\)</span> and <span class="math notranslate nohighlight">\(p_h(+1 \vert \vec{x}_{q})\)</span> the probability predicted by the hypothesis <span class="math notranslate nohighlight">\(h\)</span> for the positive class. Instead of deriving a distribution on <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> according to <a class="reference internal" href="#equation-pd">(4.6)</a>, one could also derive a predictive distribution for the (unknown) probability <span class="math notranslate nohighlight">\(q p(+1 \vert \vec{x}_q)\)</span> of the positive class:</p>
<div class="math notranslate nohighlight" id="equation-sop">
<span class="eqno">(4.8)<a class="headerlink" href="#equation-sop" title="Link to this equation">#</a></span>\[
p(q \vert \vec{x}_{q}) = 
\int_{\mathcal{H}} \, \llbracket \, p(+1 \vert \vec{x}_{q} , h) = q \, \rrbracket \, d\, p(h \vert \mathcal{D}) \enspace .
\]</div>
<p>This is a second-order probability, which still contains both aleatoric and epistemic uncertainty. The question of how to quantify the epistemic part of the uncertainty is not at all obvious, however. Intuitively, epistemic uncertainty should be reflected by the variability of the distribution <a class="reference internal" href="#equation-sop">(4.8)</a>: the more spread the probability mass over the unit interval, the higher the uncertainty seems to be. But how to put this into quantitative terms? Entropy is arguably not a good choice, for example, because this measure is invariant against redistribution of probability mass. For instance, the distributions <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(p'\)</span> with
<span class="math notranslate nohighlight">\(p(q \vert \vec{x}_{q}) 12(q-\frac{1}{2})^2\)</span> and <span class="math notranslate nohighlight">\(p'(q \vert \vec{x}_{q})  3 - p(q \vert \vec{x}_{q})\)</span> both have the same entropy, although they correspond to quite different states of information.
From this point of view, the variance of the distribution would be better suited, but this measure has other deficiencies (for example, it is not maximized by the uniform distribution, which could be considered as a case of minimal informedness).</p>
<p>The difficulty of specifying epistemic uncertainty in the Bayesian approach is rooted in the more general difficulty of representing a lack of knowledge in probability theory. This issue will be discussed next.</p>
</div></section>
<section id="representing-a-lack-of-knowledge">
<span id="rlk"></span><h2><span class="section-number">4.3. </span>Representing a lack of knowledge<a class="headerlink" href="#representing-a-lack-of-knowledge" title="Link to this heading">#</a></h2>
<div align="justify">
<p>As already said, uncertainty is captured in a purely <em>set-based</em> way in version space learning: <span class="math notranslate nohighlight">\(\mathcal{V} \subseteq \mathcal{H}\)</span> is a set of candidate hypotheses, which translates into a set of candidate outcomes <span class="math notranslate nohighlight">\(Y \subseteq \mathcal{Y}\)</span> for a query <span class="math notranslate nohighlight">\(\vec{x}_{q}\)</span>. In the case of sets, there is a rather obvious correspondence between the degree of uncertainty in the sense of a lack of knowledge and the size of the set of candidates: Proceeding from a reference set <span class="math notranslate nohighlight">\(\Omega\)</span> of alternatives, assuming some ground-truth <span class="math notranslate nohighlight">\(\omega^* \in \Omega\)</span>, and expressing knowledge about the latter in terms of a subset <span class="math notranslate nohighlight">\(C \subseteq \Omega\)</span> of possible candidates, we can clearly say that the bigger <span class="math notranslate nohighlight">\(C\)</span>, the larger the lack of knowledge. More specifically, for finite <span class="math notranslate nohighlight">\(\Omega\)</span>, a common uncertainty measure in information theory is <span class="math notranslate nohighlight">\(\log(|C|)\)</span>. Consequently, knowledge gets weaker by adding additional elements to <span class="math notranslate nohighlight">\(C\)</span> and stronger by removing candidates.</p>
<p>In standard probabilistic modeling and Bayesian inference, where knowledge is conveyed by a distribution <span class="math notranslate nohighlight">\(p\)</span> on <span class="math notranslate nohighlight">\(\Omega\)</span>, it is much less obvious how to “weaken” this knowledge. This is mainly because the total amount of belief is fixed in terms of a unit mass that can be distributed among the elements <span class="math notranslate nohighlight">\(\omega \in \Omega\)</span>. Unlike for sets, where an additional candidate <span class="math notranslate nohighlight">\(\omega\)</span> can be added or removed without changing the plausibility of all other candidates, increasing the weight of one alternative <span class="math notranslate nohighlight">\(\omega\)</span> requires decreasing the weight of another alternative <span class="math notranslate nohighlight">\(\omega'\)</span> by exactly the same amount.</p>
</div><div align="justify">
<p>Of course, there are also measures of uncertainty for probability distributions, most notably the (Shannon) entropy, which, for finite <span class="math notranslate nohighlight">\(\Omega\)</span>, is given as follows:</p>
<div class="math notranslate nohighlight" id="equation-shannon">
<span class="eqno">(4.9)<a class="headerlink" href="#equation-shannon" title="Link to this equation">#</a></span>\[
H(p) := - \sum_{\omega \in \Omega} p(\omega) \, \log p(\omega)  
\]</div>
<p>However, these measures are primarily capturing the shape of the distribution, namely its “peakedness” or non-uniformity (<span id="id4">Dubois and Hüllermeier [<a class="reference internal" href="../references.html#id298" title="D. Dubois and E. Hüllermeier. Comparing probability measures using possibility theory: a notion of relative peakedness. International Journal of Approximate Reasoning, 45(2):364–385, 2007.">DHullermeier07</a>]</span>), and hence inform about the predictability of the outcome of a random experiment. Seen from this point of view, they are more akin to aleatoric uncertainty, whereas the set-based approach (i.e., representing knowledge in terms of a subset <span class="math notranslate nohighlight">\(C \subseteq \Omega\)</span> of candidates) is arguably better suited for capturing epistemic uncertainty.</p>
</div><div align="justify">
<p>For these reasons, it has been argued that probability distributions are less suitable for representing <em>ignorance</em> in the sense of a lack of knowledge (<span id="id5">Dubois <em>et al.</em> [<a class="reference internal" href="../references.html#id1329" title="D. Dubois, H. Prade, and P. Smets. Representing partial ignorance. IEEE Transactions on Systems, Man and Cybernetics, Series A, 26(3):361-377, 1996.">DPS96</a>]</span>). For example, the case of <em>complete ignorance</em> is typically modeled in terms of the uniform distribution <span class="math notranslate nohighlight">\(p \equiv 1/|\Omega|\)</span> in probability theory; this is justified by the “principle of indifference” invoked by Laplace, or by referring to the principle of maximum entropy<a class="footnote-reference brackets" href="#footnoteidentifier2" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. Then, however, it is not possible to distinguish between (i) precise (probabilistic) knowledge about a random event, such as tossing a fair coin, and (ii) a complete lack of knowledge due to an incomplete description of the experiment.
This was already pointed out by the famous Ronald Fisher, who noted that “<em>not knowing the chance of mutually exclusive events and knowing the chance to be equal are two quite different states of knowledge</em>.”</p>
<p>Another problem in this regard is caused by the measure-theoretic grounding of probability and its additive nature. For example, the uniform distribution is not invariant under reparametrization (a uniform distribution on a parameter <span class="math notranslate nohighlight">\(\omega\)</span> does not translate into a uniform distribution on <span class="math notranslate nohighlight">\(1/\omega\)</span>, although ignorance about <span class="math notranslate nohighlight">\(\omega\)</span> implies ignorance about <span class="math notranslate nohighlight">\(1/\omega\)</span>). Likewise, expressing ignorance about the length <span class="math notranslate nohighlight">\(x\)</span> of a cube in terms of a uniform distribution on an interval <span class="math notranslate nohighlight">\([l,u]\)</span> does not yield a uniform distribution of <span class="math notranslate nohighlight">\(x^3\)</span> on <span class="math notranslate nohighlight">\([l^3 , u^3]\)</span>, thereby suggesting some degree of informedness about its volume. Problems of this kind render the use of a uniform prior distribution, often interpreted as representing epistemic uncertainty in Bayesian inference, at least debatable<a class="footnote-reference brackets" href="#footnoteidentifier3" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.</p>
</div><div align="justify">
<p>The argument that a single (probability) distribution is not enough for representing uncertain knowledge is quite prominent in the literature. Correspondingly, various generalizations of standard probability theory have been proposed, including imprecise probability (<span id="id8">Walley [<a class="reference internal" href="../references.html#id939" title="P. Walley. Statistical Reasoning with Imprecise Probabilities. Chapman and Hall, 1991.">Wal91</a>]</span>), evidence theory (<span id="id9">Shafer [<a class="reference internal" href="../references.html#id856" title="G. Shafer. A Mathematical Theory of Evidence. Princeton University Press, 1976.">Sha76</a>], Smets and Kennes [<a class="reference internal" href="../references.html#id1228" title="P. Smets and R. Kennes. The transferable belief model. Artificial Intelligence, 66:191-234, 1994.">SK94</a>]</span>), and possibility theory (<span id="id10">Dubois and Prade [<a class="reference internal" href="../references.html#id420" title="D. Dubois and H. Prade. Possibility Theory. Plenum Press, 1988.">DP88</a>]</span>). These formalisms are essentially working with sets of probability distributions, i.e., they seek to take advantage of the complementary nature of sets and distributions, and to combine both representations in a meaningful way (cf. <a class="reference internal" href="#setting2"><span class="std std-numref">Fig. 4.4</span></a>).We also refer to <a class="reference internal" href="../chapter-appendix/appendix.html#unc"><span class="std std-ref">Background on uncertainty modeling</span></a> for a brief overview.</p>
</div><figure class="align-default" id="setting2">
<a class="reference internal image-reference" href="../_images/pic-settings.png"><img alt="settings" src="../_images/pic-settings.png" style="width: 600px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.4 </span><span class="caption-text">Set-based versus distributional knowledge representation on the level of predictions, hypotheses, and models. In version space learning, the model class is fixed, and knowledge about hypotheses and outcomes is represented in terms of sets (blue color). In Bayesian inference, sets are replaced by probability distributions (gray shading). Theories like possibility and evidence theory are in-between, essentially working with sets of distributions. Credal inference (cf. Section <a class="reference internal" href="../chapter-credal_sets/credal_sets.html"><span class="doc std std-doc">Credal Sets and Classifiers</span></a> ) generalizes Bayesian inference by replacing a single prior with a set of prior distributions (here identified by a set of hyper-parameters). In hierarchical Bayesian modeling, this set is again replaced by a distribution, i.e., a second-order probability.</span><a class="headerlink" href="#setting2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footnoteidentifier1" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">1</a><span class="fn-bracket">]</span></span>
<p>Code is adapted from <a class="reference external" href="https://www.pymc.io/projects/docs/en/stable/learn/core_notebooks/GLM_linear.html#bayesian-glms-in-pymc">PyMC GLM: Linear regression</a></p>
</aside>
<aside class="footnote brackets" id="footnoteidentifier2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">2</a><span class="fn-bracket">]</span></span>
<p>Obviously, there is a technical problem in defining the uniform distribution in the case where <span class="math notranslate nohighlight">\(\Omega\)</span> is not finite.</p>
</aside>
<aside class="footnote brackets" id="footnoteidentifier3" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">3</a><span class="fn-bracket">]</span></span>
<p>This problem is inherited by hierarchical Bayesian modeling. See work on “non-informative” priors, however (<span id="id11">Jeffreys [<a class="reference internal" href="../references.html#id299" title="H. Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings of the Royal Society A, 186:453–461, 1946.">Jef46</a>]</span>,<span id="id12">Bernardo [<a class="reference internal" href="../references.html#id300" title="J.M. Bernardo. Reference posterior distributions for Bayesian inference. Journal of the Royal Statistical Society, Series B (Methodological), 41(2):113–147, 1979.">Ber79</a>]</span>).</p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./chapter-modelingAproxUncertainty"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../chapter-srcUncertainty/src.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Sources of uncertainty in supervised learning</p>
      </div>
    </a>
    <a class="right-next"
       href="../chapter-pe-scoring/scoring.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Probability Estimation via Scoring</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#version-space-learning">4.1. Version Space Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-inference">4.2. Bayesian Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-a-lack-of-knowledge">4.3. Representing a lack of knowledge</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <section>
    For comprehensive discussions please contact one of our team members: 
    {Evert.Buzon,Jiawen.Wang,Nico.Ploehn,S.Thies,Sven.Morlock,Zuo.Longfei}@campus.lmu.de

        <div style="margin-top: 50px;" id="disqus_thread"></div>

        <script>
            (function() { 
                var d = document, s = d.createElement('script');
                s.src = 'https://https-werywjw-github-io-toolbox-github-io.disqus.com/embed.js';  
                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
        })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </section> 

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>