{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-based versus distributional representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(unc)=\n",
    "## Background on uncertainty modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of uncertainty has been studied in various branches of science and scientific disciplines. For a long time, it plays a major role in fields like economics, psychology, and the social sciences, typically in the appearance of applied statistics. Likewise, its importance for artificial intelligence has been recognized very early on[^footNoteIdentifier1], at the latest with the emergence of expert systems, which came along with the need for handling inconsistency, incompleteness, imprecision, and vagueness in knowledge representation {cite:t}`krus_ua`. More recently, the phenomenon of uncertainty has also attracted a lot of attention in engineering, where it is studied under the notion of \"uncertainty quantification\"  {cite:t}`owha_ou12`; interestingly, a distinction between aleatoric and epistemic uncertainty, very much in line with our machine learning perspective, is also made there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contemporary literature on uncertainty is rather broad (cf. {numref}`calculi`). In the following, we give a brief overview, specifically focusing on the distinction between set-based and distributional (probabilistic) representations. Against the background of our discussion about aleatoric and epistemic uncertainty, this distinction is arguably important. Roughly speaking, while aleatoric uncertainty is appropriately modeled in terms of probability distributions, one may argue that a set-based approach is more suitable for modeling ignorance and a lack of knowledge, and hence more apt at capturing epistemic uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{figure-md} calculi\n",
    "<img src=\"pic-calculi.png\" alt=\"calculi\" width=\"600px\">\n",
    "\n",
    "Various uncertainty calculi and common frameworks for uncertainty representation {cite:t}`dest_up08`. Most of these formalisms are generalizations of standard probability theory an arrow denotes an \"is more general than\" relationship\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets versus distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generic way for describing situations of uncertainty is to proceed from an underlying reference set $\\Omega$, sometimes called the *frame of discernment* {cite:t}`shaf_am`. This set consists of all hypotheses, or pieces of precise information, that ought to be distinguished in the current context.  Thus, the elements $\\omega \\in \\Omega$ are exhaustive and mutually exclusive, and one of them, $\\omega^*$, corresponds to the truth. For example, $\\Omega = \\{ H, T \\}$ in the case of coin tossing, $\\Omega = \\{ \\text{win}, \\text{loss}, \\text{tie} \\}$ in predicting the outcome of a football match, or $\\Omega = \\mathbb{R} \\times \\mathbb{R}_+$ in the estimation of the parameters (expected value and standard deviation) of a normal distribution from data. For ease of exposition and to avoid measure-theoretic complications, we will subsequently assume that $\\Omega$ is a discrete (finite or countable) set. \n",
    "\n",
    "As an aside, we note that the assumption of exhaustiveness of $\\Omega$ could be relaxed. In a classification problem in machine learning, for example, not all possible classes might be known beforehand, or new classes may emerge in the course of time \\citep{hend_ab17,lian_et18,devr_lc18}. In the literature, this is often called the \"open world assumption\", whereas an exhaustive $\\Omega$ is considered as a \"closed world\" {cite:t}`deng_ge14`. Although this distinction may look technical at first sight, it has important consequences with regard to the representation and processing of uncertain information, which specifically concern the role of the empty set. While the empty set is logically excluded as a valid piece of information under the closed world assumption, it may suggest that the true state $\\omega^*$ is outside $\\Omega$ under the open world assumption.\n",
    "\n",
    "There are two basic ways for expressing uncertain information about $\\omega^*$, namely, in terms of *subsets* and in terms of *distributions*. A subset $C \\subseteq \\Omega$ corresponds to a constraint suggesting that $\\omega^* \\in C$. Thus, information or knowledge[^footNoteIdentifier2] expressed in this way distinguishes between values that are (at least provisionally) considered possible and those that are definitely excluded. As suggested by common examples such as specifying incomplete information about a numerical quantity in terms of an interval $C= [l,u]$, a set-based representation is appropriate for capturing uncertainty in the sense of *imprecision*. \n",
    "\n",
    "\n",
    "Going beyond this rough dichotomy, a distribution assigns a weight $p(\\omega)$ to each element $\\omega$, which can generally be understood as a degree of belief. At first sight, this appears to be a proper generalization of the set-based approach. Indeed, without any constraints on the weights, each subset $C$ can be characterized in terms of its indicator function $\\mathbb{I}_C$ on $\\Omega$ (which is a specific distribution assigning a weight of 1 to each $\\omega \\in C$ and 0 to all $\\omega \\not\\in \\Omega$). However, for the specifically important case of probability distributions, this view is actually not valid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, probability distributions need to obey a normalization constraint. In particular, a probability distribution requires the weights to be nonnegative and integrate to 1. A corresponding probability measure on $\\Omega$ is a set-function $\\Prob: \\, 2^\\Omega \\longrightarrow [0,1]$ such that $\\Prob(\\emptyset) = 0$, $\\Prob(\\Omega)=1$, and \n",
    "\n",
    "$$\n",
    "\\Prob(A \\cup B) = \\Prob(A) + \\Prob(B) \n",
    "$$(addi)\n",
    "\n",
    "for all disjoint sets (events) $A, B \\subseteq \\Omega$. With $\\prob(\\omega) = \\Prob(\\{ \\omega \\})$ for all $\\omega \\in \\Omega$ it follows that $\\Prob(A) = \\sum_{\\omega \\in A} \\prob(\\omega)$, and hence $\\sum_{\\omega \\in \\Omega} \\prob(\\omega)=1$. Since the set-based approach does not (need to) satify this constraint, it is no longer a special case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, in addition to the question of how information is represented, it is of course important to ask how the information is processed. In this regard, the probabilistic calculus differs fundamentally from constraint-based (set-based) information processing. The characteristic property of probability is its additivity {eq}`addi`, suggesting that the belief in the disjunction (union) of two (disjoint) events $A$ and $B$ is the sum of the belief in either of them. In contrast to this, the set-based approach is more in line with a logical interpretation and calculus. Interpreting a constraint $C$ as a logical proposition $(\\omega \\in C)$, an event $A \\subseteq \\Omega$ is possible as soon as $A \\cap C \\neq \\emptyset$ and impossible otherwise. Thus, the information $(\\omega \\in C)$ can be associated with a set-function $\\Pi:\\, 2^\\Omega \\longrightarrow \\{ 0, 1 \\}$ such that $\\Pi(A) = \\llbracket A \\cap C \\neq \\emptyset \\rrbracket$. Obviously, this set-function satisfies $\\Pi(\\emptyset) = 0$, $\\Pi(\\Omega) = 1$, and \n",
    "\n",
    "$$\n",
    "\\Pi(A \\cup B) = \\max \\big( \\Pi(A) , \\Pi(B) \\big) \n",
    "$$(maxi)\n",
    "\n",
    "Thus, $\\Pi$ is \"maxitive\" instead of additive {cite:t}`shil_mm71,dubo_pt06`. Roughly speaking, an event $A$ is evaluated according to its (logical) consistency with a constraint $C$, whereas in probability theory, an event $A$ is evaluated in terms of its probability of occurrence. The latter is reflected by the probability mass assigned to $A$, and requires a comparison of this mass with the mass of other events (since only one outcome $\\omega$ is possible, the elementary events compete with each other). Consequently, the calculus of probability, including rules for combination of information, conditioning, etc., is quite different from the corresponding calculus of constraint-based information processing {cite:t}`dubo_pt06`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(aproi)=\n",
    "## Representation of ignorance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^footNoteIdentifier1]:The \"Annual Conference on Uncertainty in Artificial Intelligence\" (UAI) was launched in the mid 1980s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^footNoteIdentifier2]:We do not distinguish between the notions of information and knowledge in this paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/x-mathjax-config\">\n",
    "    MathJax.Hub.Config({\n",
    "        TeX: {\n",
    "            Macros: {\n",
    "                llbracket: \"{\\\\lbrack\\\\lbrack}\",\n",
    "                rrbracket: \"{\\\\rbrack\\\\rbrack}\"\n",
    "            }\n",
    "        }\n",
    "    });\n",
    "</script>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
