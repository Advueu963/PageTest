{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal prediction for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conformal prediction ({cite:t}`vovk_al,shaf_at08,bala_cp`) is a framework for reliable prediction that is rooted in classical frequentist statistics, more specifically in hypothesis testing. Given a sequence of training observations and a new query $\\vec{x}_{N+1}$ (which we denoted by $\\vec{x}_{q}$ before) with unknown outcome $y_{N+1}$,\n",
    "\n",
    "$$\n",
    "(\\vec{x}_1, y_1), \\,  (\\vec{x}_2, y_2), \\ldots ,  (\\vec{x}_N, y_N), \\, (\\vec{x}_{N+1}, \\bullet)\n",
    "\\enspace ,\n",
    "$$(test)\n",
    "\n",
    "the basic idea is to hypothetically replace $\\bullet$ by each candidate, i.e., to test the hypothesis $y_{N+1} = y$ for all $y \\in \\mathcal{Y}$. Only those outcomes $y$ for which this hypothesis can be rejected at a predefined level of confidence are excluded, while those for which the hypothesis cannot be rejected are collected to form the prediction set or *prediction region* $Y^\\epsilon \\subseteq \\mathcal{Y}$. The construction of a set-valued prediction $Y^\\epsilon = Y^\\epsilon(\\vec{x}_{n+1})$ that is guaranteed to cover the true outcome $y_{N+1}$ with a given probability $1- \\epsilon$ (for example 95\\,\\%), instead of producing a point prediction $\\hat{y}_{N+1} \\in \\mathcal{Y}$, is the basic idea of conformal prediction. Here, $\\epsilon \\in (0,1)$ is a pre-specified level of significance. In the case of classification, $Y^\\epsilon$ is a subset of the set of classes $\\mathcal{Y} = \\{ y_1, \\ldots , y_K \\}$, whereas in regression, a prediction region is commonly represented in terms of an interval\\footnote{Obviously, since $\\mathcal{Y} = \\mathbb{R}$ is infinite in regression, a hypothesis test cannot be conducted explicitly for each candidate outcome $y$.}. \n",
    "\n",
    "Hypothesis testing is done in a nonparametric way: Consider any ''nonconformity'' function $f: \\, \\mathcal{X} \\times \\mathcal{Y} \\longrightarrow \\mathbb{R}$ that assigns scores $\\alpha = f(\\vec{x}, y)$ to input/output tuples; the latter can be interpreted as a measure of ''strangeness'' of the pattern $(\\vec{x}, y)$, i.e., the higher the score, the less the data point $(\\vec{x}, y)$ conforms to what one would expect to observe. Applying this function to the sequence {eq}`test`, with a specific (though hypothetical) choice of $y = y_{N+1}$, yields a sequence of scores\n",
    "\n",
    "$$\n",
    "\\alpha_1, \\, \\alpha_2, \\ldots , \\alpha_N , \\, \\alpha_{N+1} \\enspace .\n",
    "$$\n",
    "\n",
    "Denote by $\\sigma$ the permutation of $\\{1, \\ldots , N+1\\}$ that sorts the scores in increasing order, i.e., such that $\\alpha_{\\sigma(1)} \\leq \\ldots \\leq \\alpha_{\\sigma(N+1)}$. Under the assumption that the hypothetical choice of $y_{N+1}$ is in agreement with the true data-generating process, and that this process has the property of exchangeability (which is weaker than the assumption of independence and essentially means that the order of observations is irrelevant), every permutation $\\sigma$ has the same probability of occurrence. Consequently, the probability that $\\alpha_{N+1}$ is among the $\\epsilon$\\,\\% highest nonconformity scores should be low. This notion can be captured by the $p$-values associated with the candidate $y$, defined as \n",
    "\n",
    "$$\n",
    "p(y)   := \\frac{\\# \\{ i \\given \\alpha_i \\geq \\alpha_{N+1} \\}}{N+1}\n",
    "$$\n",
    "\n",
    "According to what we said, the probability that $p(y) < \\epsilon$ (i.e., $\\alpha_{N+1}$ is among the $\\epsilon$\\,\\% highest $\\alpha$-values) is upper-bounded by $\\epsilon$. \n",
    "Thus, the hypothesis $y_{N+1} = y$ can be rejected for those candidates $y$ for which $p(y) < \\epsilon$. \n",
    "\n",
    "Conformal prediction as outlined above realizes transductive inference, although inductive variants also exist \\citep{papa_ic08}.  The error bounds are \n",
    "%provided on a per-instance basis (unlike, for example, in PAC theory), and are \n",
    "valid and well calibrated by construction, regardless of the nonconformity function $f$. However, the choice of this function has an important influence on the *efficiency* of conformal prediction, that is, the size of prediction regions: The more suitable the nonconformity function is chosen, the smaller these sets will be. \n",
    "\n",
    "Although conformal prediction is mainly concerned with constructing prediction regions, the scores produced in the course of this construction can also be used for quantifying uncertainty. In this regard, the notions of *confidence* and *credibility* \\mmp{confidence and credibility} have been introduced \\citep{gam_pa02}: Let $p_1, \\ldots , p_K$ denote the $p$-values that correspond, respectively, to the candidate outcomes $y_1, \\ldots , y_K$ in a classification problem. If a definite choice (point prediction) $\\hat{y}$ has to be made, it is natural to pick the $y_i$ with the highest $p$-value. The value $p_i$ itself is then a natural measure of credibility, since the larger (closer to 1) this value, the more likely the prediction is correct. Note that the value also corresponds to the largest significance level $\\epsilon$ for which the prediction region $Y^\\epsilon$ would be empty (since all candidates would be rejected). In other words, it is a degree to which $y_i$ is indeed a plausible candidate that cannot be excluded. Another question one may ask is to what extent $y_i$ is the unique candidate of that kind, i.e., to what extent other candidates can indeed be excluded. This can be quantified in terms of the greatest $1 - \\epsilon$ for which $Y^\\epsilon$ is the singleton set $\\{ y_i \\}$, that is, $1$ minus the second-highest $p$-value. Besides, other methods for quantifying the uncertainty of a point prediction in the context of conformal prediction have been proposed \\citep{linu_rc16}. \n",
    "\n",
    "\n",
    "With its main concern of constructing valid prediction regions, conformal prediction differs from most other machine learning methods, which produce point predictions $y \\in \\mathcal{Y}$, whether equipped with a degree of uncertainty or not. In a sense, conformal prediction can even be seen as being orthogonal: It predefines the degree of uncertainty (level of confidence) and adjusts its prediction correspondingly, rather than the other way around. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
