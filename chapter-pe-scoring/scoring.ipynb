{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Estimation via Scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from properscoring import crps_ensemble\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, brier_score_loss, accuracy_score, roc_auc_score\n",
    "\n",
    "# Paper Page 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various methods in machine learning for inducing probabilistic predictors.\n",
    "These are hypotheses $h$ that do not merely output point predictions $h(\\vec{x}) \\in \\mathcal{Y}$, \n",
    "i.e., elements of the output space $\\mathcal{Y}$, \n",
    "but probability estimates $p_h(\\cdot \\vert \\vec{x}) =  p(\\cdot \\vert \\vec{x}, h)$, \n",
    "i.e., complete probability distributions on $\\mathcal{Y}$. \n",
    "In the case of classification, \n",
    "this means predicting a single (conditional) probability $p_h(y \\vert \\vec{x}) = p(y \\vert \\vec{x} , h)$ for each class $y \\in \\mathcal{Y}$, \n",
    "whereas in regression, $p( \\cdot \\vert \\vec{x}, h)$ is a density function on $\\mathbb{R}$. \n",
    "Such predictors can be learned in a discriminative way, \n",
    "i.e., in the form of a mapping $\\vec{x} \\mapsto p( \\cdot \\vert \\vec{x})$, \n",
    "or in a generative way, which essentially means learning a joint distribution on $\\mathcal{X} \\times \\mathcal{Y}$. \n",
    "Moreover, the approaches can be parametric (assuming specific parametric families of probability distributions) or non-parametric. \n",
    "Well-known examples include classical statistical methods such as logistic and linear regression, \n",
    "Bayesian approaches such as Bayesian networks and Gaussian processes, <!-- (cf.\\ Section \\ref{sec:gp}),  -->\n",
    "as well as various techniques in the realm of (deep) neural networks. \n",
    "<!-- (cf.\\ Section \\ref{sec:m1}).  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example: Logistic Regression\n",
    "\n",
    "Logistic regression is a widely used supervised machine learning method for binary classification that provides probabilistic outputs. \n",
    "Below is an example using `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict_proba(X_test)[:, 1]\n",
    "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Example: Linear Regression\n",
    "\n",
    "In statistics, linear regression is a statistical model used for predicting a continuous target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0107\n"
     ]
    }
   ],
   "source": [
    "## Example: Linear Regression\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: Bayesian networks (UAIML: Slides 31&32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training probabilistic predictors is typically accomplished by minimizing suitable loss functions, \n",
    "i.e., loss functions that enforce \"correct\" (conditional) probabilities as predictions. \n",
    "In this regard, \n",
    "proper scoring rules ({cite:t}`gnei_sp05`) <!-- \\citep{gnei_sp05}  -->\n",
    "play an important role, \n",
    "including the log-loss as a well-known special case. \n",
    "Sometimes, however, estimates are also obtained in a very simple way, \n",
    "following basic frequentist techniques for probability estimation, \n",
    "like in Naïve Bayes or nearest neighbor classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example: Naïve Bayes (UAIML: Slides 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logarithmic Scoring Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Score: 0.7136\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([1, 1, 0])\n",
    "\n",
    "p_pred = np.array([0.8, 0.3, 0.6])\n",
    "\n",
    "log_score = -np.mean(np.log(p_pred[y_true == 1]))\n",
    "print(f\"Log Score: {log_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brier Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brier Score: 0.2967\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([1, 1, 0])\n",
    "\n",
    "p_pred = np.array([0.8, 0.3, 0.6])\n",
    "\n",
    "brier_score = brier_score_loss(y_true, p_pred)\n",
    "print(f\"Brier Score: {brier_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Ranked Probability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Ranked Probability Score: 0.1000\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([3.5])\n",
    "\n",
    "predicted_ensemble = np.array([[3.0, 3.2, 3.4, 3.6, 3.8]])\n",
    "\n",
    "crps_score = crps_ensemble(y_true, predicted_ensemble)\n",
    "print(f\"Continuous Ranked Probability Score: {crps_score.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
