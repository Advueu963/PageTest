{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Estimation via Scoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various methods in machine learning for inducing probabilistic predictors.\n",
    "These are hypotheses $h$ that do not merely output point predictions $h(\\vec{x}) \\in \\mathcal{Y}$, \n",
    "i.e., elements of the output space $\\mathcal{Y}$, \n",
    "but probability estimates $p_h(\\cdot \\vert \\vec{x}) =  p(\\cdot \\vert \\vec{x}, h)$, \n",
    "i.e., complete probability distributions on $\\mathcal{Y}$. \n",
    "In the case of classification, \n",
    "this means predicting a single (conditional) probability $p_h(y \\vert \\vec{x}) = p(y \\vert \\vec{x} , h)$ for each class $y \\in \\mathcal{Y}$, \n",
    "whereas in regression, $p( \\cdot \\vert \\vec{x}, h)$ is a density function on $\\mathbb{R}$. \n",
    "Such predictors can be learned in a discriminative way, \n",
    "i.e., in the form of a mapping $\\vec{x} \\mapsto p( \\cdot \\vert \\vec{x})$, \n",
    "or in a generative way, which essentially means learning a joint distribution on $\\mathcal{X} \\times \\mathcal{Y}$. \n",
    "Moreover, the approaches can be parametric (assuming specific parametric families of probability distributions) or non-parametric. \n",
    "Well-known examples include classical statistical methods such as logistic and linear regression, \n",
    "Bayesian approaches such as Bayesian networks and Gaussian processes, <!-- (cf.\\ Section \\ref{sec:gp}),  -->\n",
    "as well as various techniques in the realm of (deep) neural networks. \n",
    "<!-- (cf.\\ Section \\ref{sec:m1}).  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training probabilistic predictors is typically accomplished by minimizing suitable loss functions, \n",
    "i.e., loss functions that enforce \"correct\" (conditional) probabilities as predictions. \n",
    "In this regard, \n",
    "proper scoring rules ({cite:t}`gnei_sp05`) <!-- \\citep{gnei_sp05}  -->\n",
    "play an important role, \n",
    "including the log-loss as a well-known special case. \n",
    "Sometimes, however, estimates are also obtained in a very simple way, \n",
    "following basic frequentist techniques for probability estimation, \n",
    "like in Na√Øve Bayes or nearest neighbor classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions delivered by corresponding methods are at best \"pseudo-probabilities\" that are often not very accurate. \n",
    "Besides, there are many methods that deliver natural scores, \n",
    "intuitively expressing a degree of confidence \n",
    "(like the distance from the separating hyperplane in support vector machines), \n",
    "but which do not immediately qualify as probabilities either. \n",
    "The idea of *scaling* or *calibration methods* is to turn such scores into proper, \n",
    "well-calibrated probabilities, that is, \n",
    "to learn a mapping from scores to the unit interval that can be applied to the output of a predictor as a kind of post-processing step ({cite:t}`flac_cc17`). <!-- \\citep{flac_cc17} -->\n",
    "Examples of such methods include binning ({cite:p}`zadr_oc01`), <!-- \\citep{zadr_oc01} -->\n",
    "isotonic regression ({cite:t}`zadr_tc02`), <!-- \\citep{zadr_tc02} -->\n",
    "logistic scaling \\citep{Pla00} and improvements thereof ({cite:p}`kull_bc17`), <!-- \\citep{kull_bc17} -->\n",
    "as well as the use of Venn predictors ({cite:p}`joha_vp18`). <!-- \\citep{joha_vp18} -->\n",
    "Calibration is still a topic of ongoing research. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another import class of methods is *ensemble learning*, such as bagging or boosting, which are especially popular in machine learning due to their ability to improve accuracy of (point) predictions. \n",
    "Since such methods produce a (large) set of predictors $h_1, \\ldots, h_M$ instead of a single hypothesis, it is tempting to produce probability estimates following basic frequentist principles. In the simplest case (of classification), each prediction $h_i(\\vec{x})$ can be interpreted as a \"vote\" in favor of a class $y \\in \\mathcal{Y}$, and probabilities can be estimated by relative frequencies\\,---\\,needless to say, probabilities constructed in this way tend to be biased and are not necessarily well calibrated. Especially important in this field are tree-based methods such as random forests ({cite:t}`brei_rf01,krup_pe14`).  \n",
    "<!-- \\citep{brei_rf01,krup_pe14} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, while standard probability estimation is a viable approach to representing uncertainty in a prediction, \n",
    "there is no explicit distinction between different types of uncertainty. \n",
    "Methods falling into this category are mostly concerned with the aleatoric part of the overall uncertainty.\n",
    "\\footnote{Yet, as will be seen later on, one way to go beyond mere aleatoric uncertainty is to combine the above methods, for example learning ensembles of probabilistic predictors (cf.\\ Section \\ref{sec:m1}).}  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logarithmic Scoring Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Score: 0.7136\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 1, 0])\n",
    "\n",
    "p_pred = np.array([0.8, 0.3, 0.6])\n",
    "\n",
    "log_score = -np.mean(np.log(p_pred[y_true == 1]))\n",
    "print(f\"Log Score: {log_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Brier Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brier Score: 0.2967\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "y_true = np.array([1, 1, 0])\n",
    "\n",
    "p_pred = np.array([0.8, 0.3, 0.6])\n",
    "\n",
    "brier_score = brier_score_loss(y_true, p_pred)\n",
    "print(f\"Brier Score: {brier_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Continuous Ranked Probability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous Ranked Probability Score: 0.1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from properscoring import crps_ensemble\n",
    "\n",
    "y_true = np.array([3.5])\n",
    "\n",
    "predicted_ensemble = np.array([[3.0, 3.2, 3.4, 3.6, 3.8]])\n",
    "\n",
    "crps_score = crps_ensemble(y_true, predicted_ensemble)\n",
    "print(f\"Continuous Ranked Probability Score: {crps_score.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
